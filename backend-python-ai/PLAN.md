å¾ˆå¥½ï¼çŸ¥è¯†åº“æœåŠ¡å¼€å‘æ˜¯AIç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ã€‚è®©æˆ‘ä¸ºä½ åˆ¶å®šè¯¦ç»†çš„å¼€å‘æ­¥éª¤ï¼š

## ğŸ“‹ çŸ¥è¯†åº“æœåŠ¡å¼€å‘è¯¦ç»†æ­¥éª¤

### ğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¶æ„ä¸æ•°æ®æ¨¡å‹ï¼ˆ1-2å¤©ï¼‰

#### 1. å®Œå–„æ•°æ®æ¨¡å‹
**æ–‡ä»¶ï¼š`app/models/schemas.py`ï¼ˆè¡¥å……ï¼‰**

```python
# ==================== çŸ¥è¯†åº“ç›¸å…³æ¨¡å‹è¡¥å…… ====================

class KnowledgeBaseCreate(BaseModel):
    """çŸ¥è¯†åº“åˆ›å»ºæ¨¡å‹"""
    name: str = Field(..., min_length=1, max_length=100, description="çŸ¥è¯†åº“åç§°")
    description: Optional[str] = Field(None, max_length=500, description="æè¿°")
    tags: Optional[List[str]] = Field(default_factory=list, description="æ ‡ç­¾")
    is_public: bool = Field(default=True, description="æ˜¯å¦å…¬å¼€")
    category: Optional[str] = Field(None, description="åˆ†ç±»")

class KnowledgeBaseUpdate(BaseModel):
    """çŸ¥è¯†åº“æ›´æ–°æ¨¡å‹"""
    name: Optional[str] = Field(None, min_length=1, max_length=100, description="çŸ¥è¯†åº“åç§°")
    description: Optional[str] = Field(None, max_length=500, description="æè¿°")
    tags: Optional[List[str]] = Field(None, description="æ ‡ç­¾")
    is_public: Optional[bool] = Field(None, description="æ˜¯å¦å…¬å¼€")
    status: Optional[str] = Field(None, description="çŠ¶æ€")

class KnowledgeBaseResponse(BaseModel):
    """çŸ¥è¯†åº“å“åº”æ¨¡å‹"""
    id: str = Field(..., description="çŸ¥è¯†åº“ID")
    name: str = Field(..., description="çŸ¥è¯†åº“åç§°")
    description: Optional[str] = Field(None, description="æè¿°")
    doc_count: int = Field(default=0, description="æ–‡æ¡£æ•°é‡")
    created_at: datetime = Field(..., description="åˆ›å»ºæ—¶é—´")
    updated_at: datetime = Field(..., description="æ›´æ–°æ—¶é—´")
    created_by: str = Field(..., description="åˆ›å»ºè€…ID")
    status: str = Field(default="active", description="çŠ¶æ€: active/inactive/processing")
    tags: List[str] = Field(default_factory=list, description="æ ‡ç­¾")
    is_public: bool = Field(default=True, description="æ˜¯å¦å…¬å¼€")
    vectorized: bool = Field(default=False, description="æ˜¯å¦å·²å‘é‡åŒ–")
    category: Optional[str] = Field(None, description="åˆ†ç±»")

    class Config:
        from_attributes = True

class DocumentUploadRequest(BaseModel):
    """æ–‡æ¡£ä¸Šä¼ è¯·æ±‚æ¨¡å‹"""
    knowledge_base_id: str = Field(..., description="çŸ¥è¯†åº“ID")
    file_name: str = Field(..., description="æ–‡ä»¶å")
    file_type: str = Field(..., description="æ–‡ä»¶ç±»å‹")
    chunk_size: int = Field(default=1000, description="åˆ†å—å¤§å°")
    chunk_overlap: int = Field(default=200, description="åˆ†å—é‡å ")

class DocumentItem(BaseModel):
    """æ–‡æ¡£é¡¹æ¨¡å‹"""
    id: str = Field(..., description="æ–‡æ¡£é¡¹ID")
    knowledge_base_id: str = Field(..., description="çŸ¥è¯†åº“ID")
    content: str = Field(..., description="å†…å®¹")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="å…ƒæ•°æ®")
    source_file: Optional[str] = Field(None, description="æºæ–‡ä»¶")
    page_number: Optional[int] = Field(None, description="é¡µç ")
    chunk_index: int = Field(..., description="åˆ†å—ç´¢å¼•")
    total_chunks: int = Field(..., description="æ€»åˆ†å—æ•°")
    vector_id: Optional[str] = Field(None, description="å‘é‡ID")
    created_at: datetime = Field(..., description="åˆ›å»ºæ—¶é—´")
    updated_at: datetime = Field(..., description="æ›´æ–°æ—¶é—´")

class KnowledgeQuery(BaseModel):
    """çŸ¥è¯†æŸ¥è¯¢æ¨¡å‹"""
    query: str = Field(..., min_length=1, max_length=1000, description="æŸ¥è¯¢å†…å®¹")
    knowledge_base_id: str = Field(..., description="çŸ¥è¯†åº“ID")
    top_k: int = Field(default=5, ge=1, le=20, description="è¿”å›ç»“æœæ•°é‡")
    score_threshold: float = Field(default=0.7, ge=0.0, le=1.0, description="ç›¸ä¼¼åº¦é˜ˆå€¼")
    include_metadata: bool = Field(default=True, description="æ˜¯å¦åŒ…å«å…ƒæ•°æ®")

class SearchResult(BaseModel):
    """æœç´¢ç»“æœæ¨¡å‹"""
    content: str = Field(..., description="å†…å®¹")
    score: float = Field(..., description="ç›¸ä¼¼åº¦åˆ†æ•°")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="å…ƒæ•°æ®")
    source_file: Optional[str] = Field(None, description="æºæ–‡ä»¶")
    page_number: Optional[int] = Field(None, description="é¡µç ")
```

#### 2. åˆ›å»ºçŸ¥è¯†åº“æœåŠ¡
**æ–‡ä»¶ï¼š`app/services/knowledge_service.py`**

```python
"""
çŸ¥è¯†åº“ç®¡ç†æœåŠ¡
å¤„ç†çŸ¥è¯†åº“çš„åˆ›å»ºã€æ–‡æ¡£ä¸Šä¼ ã€å‘é‡åŒ–ç­‰ä¸šåŠ¡é€»è¾‘
"""

import uuid
import json
import os
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from pathlib import Path

from app.utils.logger import LoggerMixin
from app.models.schemas import (
    KnowledgeBaseCreate, 
    KnowledgeBaseUpdate, 
    KnowledgeBaseResponse,
    DocumentItem
)

class KnowledgeService(LoggerMixin):
    """
    çŸ¥è¯†åº“ç®¡ç†æœåŠ¡
    å¤„ç†çŸ¥è¯†åº“çš„CRUDã€æ–‡æ¡£ç®¡ç†ã€å‘é‡åŒ–ç­‰
    """
    
    def __init__(self, data_dir: str = "./data"):
        """åˆå§‹åŒ–çŸ¥è¯†åº“æœåŠ¡"""
        super().__init__()
        
        # å†…å­˜å­˜å‚¨ï¼ˆåç»­æ›¿æ¢ä¸ºæ•°æ®åº“ï¼‰
        self._knowledge_bases: Dict[str, Dict[str, Any]] = {}
        self._documents: Dict[str, List[Dict[str, Any]]] = {}
        
        # æ–‡ä»¶å­˜å‚¨ç›®å½•
        self.data_dir = Path(data_dir)
        self.upload_dir = self.data_dir / "uploads"
        self.vector_db_dir = self.data_dir / "vector_db"
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.upload_dir.mkdir(parents=True, exist_ok=True)
        self.vector_db_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç¤ºä¾‹æ•°æ®
        self._init_sample_data()
        
        self.log_info("çŸ¥è¯†åº“æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _init_sample_data(self):
        """åˆå§‹åŒ–ç¤ºä¾‹çŸ¥è¯†åº“æ•°æ®"""
        
        sample_knowledge_bases = [
            {
                "id": "kb_001",
                "name": "äº§å“æ‰‹å†Œ",
                "description": "åŒ…å«æ‰€æœ‰äº§å“åŠŸèƒ½å’Œä½¿ç”¨è¯´æ˜",
                "doc_count": 3,
                "created_by": "system",
                "created_at": datetime(2024, 1, 1, 10, 0, 0),
                "updated_at": datetime(2024, 1, 15, 14, 30, 0),
                "status": "active",
                "tags": ["äº§å“", "æ‰‹å†Œ", "ä½¿ç”¨è¯´æ˜"],
                "is_public": True,
                "vectorized": True,
                "category": "äº§å“æ–‡æ¡£"
            },
            {
                "id": "kb_002",
                "name": "æŠ€æœ¯æ”¯æŒæ–‡æ¡£",
                "description": "å¸¸è§æŠ€æœ¯é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ",
                "doc_count": 2,
                "created_by": "system",
                "created_at": datetime(2024, 1, 5, 9, 0, 0),
                "updated_at": datetime(2024, 1, 20, 11, 0, 0),
                "status": "active",
                "tags": ["æŠ€æœ¯", "æ”¯æŒ", "FAQ"],
                "is_public": True,
                "vectorized": False,
                "category": "æŠ€æœ¯æ”¯æŒ"
            }
        ]
        
        for kb in sample_knowledge_bases:
            self._knowledge_bases[kb["id"]] = kb
            self._documents[kb["id"]] = []
        
        # æ·»åŠ ç¤ºä¾‹æ–‡æ¡£
        sample_documents = [
            {
                "id": "doc_001",
                "knowledge_base_id": "kb_001",
                "content": "äº§å“åŠŸèƒ½ä»‹ç»ï¼šæˆ‘ä»¬çš„AIåŠ©æ‰‹æä¾›æ™ºèƒ½å¯¹è¯ã€æ–‡æ¡£åˆ†æã€çŸ¥è¯†æ£€ç´¢ç­‰åŠŸèƒ½ã€‚",
                "metadata": {"type": "ä»‹ç»", "importance": "é«˜"},
                "source_file": "äº§å“ä»‹ç».pdf",
                "page_number": 1,
                "chunk_index": 0,
                "total_chunks": 1,
                "vector_id": "vec_001",
                "created_at": datetime(2024, 1, 2, 10, 0, 0),
                "updated_at": datetime(2024, 1, 2, 10, 0, 0)
            },
            {
                "id": "doc_002",
                "knowledge_base_id": "kb_001",
                "content": "å®‰è£…æ­¥éª¤ï¼š1. ä¸‹è½½å®‰è£…åŒ… 2. è¿è¡Œå®‰è£…ç¨‹åº 3. é…ç½®åŸºæœ¬è®¾ç½® 4. å¼€å§‹ä½¿ç”¨ã€‚",
                "metadata": {"type": "æŒ‡å—", "difficulty": "ç®€å•"},
                "source_file": "å®‰è£…æŒ‡å—.pdf",
                "page_number": 1,
                "chunk_index": 0,
                "total_chunks": 1,
                "vector_id": "vec_002",
                "created_at": datetime(2024, 1, 3, 11, 0, 0),
                "updated_at": datetime(2024, 1, 3, 11, 0, 0)
            }
        ]
        
        for doc in sample_documents:
            kb_id = doc["knowledge_base_id"]
            if kb_id in self._documents:
                self._documents[kb_id].append(doc)
    
    def create_knowledge_base(self, kb_data: KnowledgeBaseCreate, created_by: str) -> KnowledgeBaseResponse:
        """
        åˆ›å»ºæ–°çŸ¥è¯†åº“
        
        Args:
            kb_data: çŸ¥è¯†åº“æ•°æ®
            created_by: åˆ›å»ºè€…ID
            
        Returns:
            KnowledgeBaseResponse: åˆ›å»ºçš„çŸ¥è¯†åº“
        """
        
        try:
            # ç”ŸæˆçŸ¥è¯†åº“ID
            kb_id = f"kb_{str(uuid.uuid4())[:8]}"
            
            now = datetime.now()
            
            # åˆ›å»ºçŸ¥è¯†åº“è®°å½•
            kb_record = {
                "id": kb_id,
                **kb_data.dict(),
                "doc_count": 0,
                "created_by": created_by,
                "created_at": now,
                "updated_at": now,
                "status": "active",
                "vectorized": False
            }
            
            # ä¿å­˜åˆ°å†…å­˜å­˜å‚¨
            self._knowledge_bases[kb_id] = kb_record
            self._documents[kb_id] = []
            
            # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
            kb_dir = self.upload_dir / kb_id
            kb_dir.mkdir(parents=True, exist_ok=True)
            
            self.log_info(f"åˆ›å»ºçŸ¥è¯†åº“æˆåŠŸ: {kb_id}, åç§°: {kb_data.name}")
            
            return KnowledgeBaseResponse(**kb_record)
            
        except Exception as e:
            self.log_error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}", error=e)
            raise
    
    def get_knowledge_base(self, kb_id: str) -> Optional[KnowledgeBaseResponse]:
        """
        è·å–çŸ¥è¯†åº“è¯¦æƒ…
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            
        Returns:
            Optional[KnowledgeBaseResponse]: çŸ¥è¯†åº“ä¿¡æ¯
        """
        
        if kb_id not in self._knowledge_bases:
            self.log_warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_id}")
            return None
        
        return KnowledgeBaseResponse(**self._knowledge_bases[kb_id])
    
    def update_knowledge_base(self, kb_id: str, update_data: KnowledgeBaseUpdate) -> Optional[KnowledgeBaseResponse]:
        """
        æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            update_data: æ›´æ–°æ•°æ®
            
        Returns:
            Optional[KnowledgeBaseResponse]: æ›´æ–°åçš„çŸ¥è¯†åº“ä¿¡æ¯
        """
        
        if kb_id not in self._knowledge_bases:
            self.log_warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°: {kb_id}")
            return None
        
        try:
            # è·å–ç°æœ‰çŸ¥è¯†åº“æ•°æ®
            kb = self._knowledge_bases[kb_id]
            
            # åº”ç”¨æ›´æ–°
            update_dict = update_data.dict(exclude_unset=True)
            
            for key, value in update_dict.items():
                if value is not None:
                    kb[key] = value
            
            # æ›´æ–°æ›´æ–°æ—¶é—´
            kb["updated_at"] = datetime.now()
            
            self.log_info(f"æ›´æ–°çŸ¥è¯†åº“æˆåŠŸ: {kb_id}")
            
            return KnowledgeBaseResponse(**kb)
            
        except Exception as e:
            self.log_error(f"æ›´æ–°çŸ¥è¯†åº“å¤±è´¥: {kb_id}, é”™è¯¯: {str(e)}", error=e)
            return None
    
    def delete_knowledge_base(self, kb_id: str, user_id: str) -> bool:
        """
        åˆ é™¤çŸ¥è¯†åº“
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºæƒé™æ£€æŸ¥ï¼‰
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        
        if kb_id not in self._knowledge_bases:
            self.log_warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤: {kb_id}")
            return False
        
        try:
            # æ£€æŸ¥æƒé™ï¼ˆä»…åˆ›å»ºè€…å¯ä»¥åˆ é™¤ï¼‰
            kb = self._knowledge_bases[kb_id]
            if kb["created_by"] != user_id:
                self.log_warning(f"ç”¨æˆ· {user_id} æ— æƒåˆ é™¤çŸ¥è¯†åº“ {kb_id}")
                return False
            
            # åˆ é™¤çŸ¥è¯†åº“ç›®å½•
            kb_dir = self.upload_dir / kb_id
            if kb_dir.exists():
                import shutil
                shutil.rmtree(kb_dir)
            
            # åˆ é™¤å†…å­˜ä¸­çš„æ•°æ®
            del self._knowledge_bases[kb_id]
            if kb_id in self._documents:
                del self._documents[kb_id]
            
            self.log_info(f"åˆ é™¤çŸ¥è¯†åº“æˆåŠŸ: {kb_id}")
            return True
            
        except Exception as e:
            self.log_error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {kb_id}, é”™è¯¯: {str(e)}", error=e)
            return False
    
    def list_knowledge_bases(
        self, 
        user_id: Optional[str] = None,
        status: Optional[str] = None,
        is_public: Optional[bool] = None,
        limit: int = 20,
        offset: int = 0
    ) -> List[KnowledgeBaseResponse]:
        """
        åˆ—å‡ºçŸ¥è¯†åº“
        
        Args:
            user_id: ç”¨æˆ·IDè¿‡æ»¤ï¼ˆåˆ›å»ºè€…ï¼‰
            status: çŠ¶æ€è¿‡æ»¤
            is_public: æ˜¯å¦å…¬å¼€è¿‡æ»¤
            limit: è¿”å›æ•°é‡é™åˆ¶
            offset: åç§»é‡
            
        Returns:
            List[KnowledgeBaseResponse]: çŸ¥è¯†åº“åˆ—è¡¨
        """
        
        try:
            filtered_kbs = []
            
            for kb_id, kb_data in self._knowledge_bases.items():
                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if user_id and kb_data.get("created_by") != user_id:
                    # å¦‚æœç”¨æˆ·ä¸æ˜¯åˆ›å»ºè€…ï¼Œåªæ˜¾ç¤ºå…¬å¼€çš„çŸ¥è¯†åº“
                    if not kb_data.get("is_public", True):
                        continue
                
                if status and kb_data.get("status") != status:
                    continue
                
                if is_public is not None and kb_data.get("is_public") != is_public:
                    continue
                
                filtered_kbs.append(KnowledgeBaseResponse(**kb_data))
            
            # æŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åº
            filtered_kbs.sort(key=lambda x: x.updated_at, reverse=True)
            
            # åº”ç”¨åˆ†é¡µ
            start_idx = offset
            end_idx = offset + limit
            paginated_kbs = filtered_kbs[start_idx:end_idx]
            
            self.log_debug(f"åˆ—å‡ºçŸ¥è¯†åº“ - è¿‡æ»¤åæ•°é‡: {len(filtered_kbs)}, åˆ†é¡µå: {len(paginated_kbs)}")
            
            return paginated_kbs
            
        except Exception as e:
            self.log_error(f"åˆ—å‡ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}", error=e)
            return []
    
    def add_document(self, kb_id: str, document_data: Dict[str, Any]) -> Optional[DocumentItem]:
        """
        æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            document_data: æ–‡æ¡£æ•°æ®
            
        Returns:
            Optional[DocumentItem]: æ·»åŠ çš„æ–‡æ¡£é¡¹
        """
        
        if kb_id not in self._knowledge_bases:
            self.log_warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œæ— æ³•æ·»åŠ æ–‡æ¡£: {kb_id}")
            return None
        
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = f"doc_{str(uuid.uuid4())[:8]}"
            now = datetime.now()
            
            # åˆ›å»ºæ–‡æ¡£è®°å½•
            document_record = {
                "id": doc_id,
                "knowledge_base_id": kb_id,
                "created_at": now,
                "updated_at": now,
                **document_data
            }
            
            # æ·»åŠ åˆ°å†…å­˜å­˜å‚¨
            self._documents[kb_id].append(document_record)
            
            # æ›´æ–°çŸ¥è¯†åº“æ–‡æ¡£è®¡æ•°
            self._knowledge_bases[kb_id]["doc_count"] = len(self._documents[kb_id])
            self._knowledge_bases[kb_id]["updated_at"] = now
            
            self.log_info(f"æ·»åŠ æ–‡æ¡£æˆåŠŸ: {doc_id}, çŸ¥è¯†åº“: {kb_id}")
            
            return DocumentItem(**document_record)
            
        except Exception as e:
            self.log_error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {kb_id}, é”™è¯¯: {str(e)}", error=e)
            return None
    
    def get_documents(self, kb_id: str, limit: int = 50, offset: int = 0) -> List[DocumentItem]:
        """
        è·å–çŸ¥è¯†åº“çš„æ–‡æ¡£åˆ—è¡¨
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            limit: è¿”å›æ•°é‡é™åˆ¶
            offset: åç§»é‡
            
        Returns:
            List[DocumentItem]: æ–‡æ¡£åˆ—è¡¨
        """
        
        if kb_id not in self._documents:
            return []
        
        try:
            documents = self._documents[kb_id]
            
            # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åº
            documents.sort(key=lambda x: x.get("created_at", datetime.min), reverse=True)
            
            # åº”ç”¨åˆ†é¡µ
            start_idx = offset
            end_idx = offset + limit
            paginated_docs = documents[start_idx:end_idx]
            
            return [DocumentItem(**doc) for doc in paginated_docs]
            
        except Exception as e:
            self.log_error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {kb_id}, é”™è¯¯: {str(e)}", error=e)
            return []
    
    def search_documents(self, kb_id: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢æ–‡æ¡£ï¼ˆç®€å•æ–‡æœ¬åŒ¹é…ï¼Œåç»­æ›¿æ¢ä¸ºå‘é‡æœç´¢ï¼‰
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            query: æŸ¥è¯¢å†…å®¹
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æœç´¢ç»“æœ
        """
        
        if kb_id not in self._documents:
            return []
        
        try:
            results = []
            query_lower = query.lower()
            
            for doc in self._documents[kb_id]:
                content = doc.get("content", "")
                if query_lower in content.lower():
                    # ç®€å•çš„åŒ¹é…åˆ†æ•°è®¡ç®—
                    score = min(1.0, len(query) / len(content) * 10) if content else 0.0
                    
                    results.append({
                        "content": content[:200] + "..." if len(content) > 200 else content,
                        "score": score,
                        "metadata": doc.get("metadata", {}),
                        "source_file": doc.get("source_file"),
                        "page_number": doc.get("page_number")
                    })
            
            # æŒ‰åˆ†æ•°æ’åº
            results.sort(key=lambda x: x["score"], reverse=True)
            
            return results[:top_k]
            
        except Exception as e:
            self.log_error(f"æœç´¢æ–‡æ¡£å¤±è´¥: {kb_id}, é”™è¯¯: {str(e)}", error=e)
            return []

# åˆ›å»ºå…¨å±€çŸ¥è¯†åº“æœåŠ¡å®ä¾‹
knowledge_service = KnowledgeService()
```

### ğŸ¯ ç¬¬äºŒé˜¶æ®µï¼šæ–‡æ¡£å¤„ç†æœåŠ¡ï¼ˆ2-3å¤©ï¼‰

#### 3. åˆ›å»ºæ–‡æ¡£è§£æå™¨
**æ–‡ä»¶ï¼š`app/services/processing/document_parser.py`**

```python
"""
æ–‡æ¡£è§£ææœåŠ¡
è§£æPDFã€Wordã€TXTç­‰æ ¼å¼çš„æ–‡æ¡£
"""

import io
import re
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path

try:
    import PyPDF2
    HAS_PYPDF2 = True
except ImportError:
    HAS_PYPDF2 = False

try:
    from docx import Document as DocxDocument
    HAS_DOCX = True
except ImportError:
    HAS_DOCX = False

from app.utils.logger import LoggerMixin

class DocumentParser(LoggerMixin):
    """
    æ–‡æ¡£è§£æå™¨
    æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„è§£æ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡æ¡£è§£æå™¨"""
        super().__init__()
        self.log_info("æ–‡æ¡£è§£æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def parse_document(self, file_path: Path, file_type: str) -> Dict[str, Any]:
        """
        è§£ææ–‡æ¡£
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹ (pdf, docx, txt, md)
            
        Returns:
            Dict: è§£æç»“æœï¼ŒåŒ…å«å†…å®¹å’Œå…ƒæ•°æ®
        """
        
        self.log_info(f"å¼€å§‹è§£ææ–‡æ¡£: {file_path}, ç±»å‹: {file_type}")
        
        try:
            if not file_path.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            if file_type.lower() == 'pdf':
                return self._parse_pdf(file_path)
            elif file_type.lower() == 'docx':
                return self._parse_docx(file_path)
            elif file_type.lower() in ['txt', 'md', 'markdown']:
                return self._parse_text(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
                
        except Exception as e:
            self.log_error(f"è§£ææ–‡æ¡£å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}", error=e)
            raise
    
    def _parse_pdf(self, file_path: Path) -> Dict[str, Any]:
        """è§£æPDFæ–‡ä»¶"""
        
        if not HAS_PYPDF2:
            raise ImportError("è¯·å®‰è£… PyPDF2: pip install PyPDF2")
        
        try:
            content_parts = []
            metadata = {}
            
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # è·å–æ–‡æ¡£ä¿¡æ¯
                if pdf_reader.metadata:
                    metadata = {
                        'title': pdf_reader.metadata.get('/Title', ''),
                        'author': pdf_reader.metadata.get('/Author', ''),
                        'subject': pdf_reader.metadata.get('/Subject', ''),
                        'creator': pdf_reader.metadata.get('/Creator', ''),
                        'producer': pdf_reader.metadata.get('/Producer', ''),
                        'creation_date': pdf_reader.metadata.get('/CreationDate', ''),
                        'modification_date': pdf_reader.metadata.get('/ModDate', ''),
                    }
                
                # æå–æ¯é¡µæ–‡æœ¬
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    
                    # æ¸…ç†æ–‡æœ¬
                    cleaned_text = self._clean_text(page_text)
                    if cleaned_text:
                        content_parts.append({
                            'page_number': page_num,
                            'content': cleaned_text
                        })
            
            # åˆå¹¶æ‰€æœ‰é¡µé¢å†…å®¹
            full_content = "\n\n".join([p['content'] for p in content_parts])
            
            return {
                'content': full_content,
                'metadata': metadata,
                'page_count': len(pdf_reader.pages),
                'sections': content_parts,
                'file_type': 'pdf',
                'file_size': file_path.stat().st_size
            }
            
        except Exception as e:
            self.log_error(f"è§£æPDFå¤±è´¥: {file_path}, é”™è¯¯: {str(e)}", error=e)
            raise
    
    def _parse_docx(self, file_path: Path) -> Dict[str, Any]:
        """è§£æWordæ–‡æ¡£"""
        
        if not HAS_DOCX:
            raise ImportError("è¯·å®‰è£… python-docx: pip install python-docx")
        
        try:
            content_parts = []
            doc = DocxDocument(file_path)
            
            # æå–æ®µè½
            for para_num, paragraph in enumerate(doc.paragraphs, 1):
                if paragraph.text.strip():
                    content_parts.append({
                        'paragraph_number': para_num,
                        'content': paragraph.text.strip(),
                        'style': paragraph.style.name if paragraph.style else 'Normal'
                    })
            
            # æå–è¡¨æ ¼
            tables = []
            for table_num, table in enumerate(doc.tables, 1):
                table_data = []
                for row in table.rows:
                    row_data = [cell.text.strip() for cell in row.cells]
                    table_data.append(row_data)
                tables.append(table_data)
            
            # åˆå¹¶å†…å®¹
            full_content = "\n".join([p['content'] for p in content_parts])
            
            # æ·»åŠ è¡¨æ ¼å†…å®¹
            for table in tables:
                table_text = "\n".join(["\t".join(row) for row in table])
                full_content += "\n\nè¡¨æ ¼:\n" + table_text
            
            return {
                'content': full_content,
                'metadata': {
                    'paragraph_count': len(content_parts),
                    'table_count': len(tables)
                },
                'sections': content_parts,
                'tables': tables,
                'file_type': 'docx',
                'file_size': file_path.stat().st_size
            }
            
        except Exception as e:
            self.log_error(f"è§£æWordæ–‡æ¡£å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}", error=e)
            raise
    
    def _parse_text(self, file_path: Path) -> Dict[str, Any]:
        """è§£ææ–‡æœ¬æ–‡ä»¶"""
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            # æ¸…ç†æ–‡æœ¬
            cleaned_content = self._clean_text(content)
            
            # åˆ†ææ–‡æœ¬ç»“æ„
            lines = content.split('\n')
            sections = []
            current_section = []
            section_num = 1
            
            for line_num, line in enumerate(lines, 1):
                if line.strip():
                    current_section.append(line.strip())
                else:
                    if current_section:
                        sections.append({
                            'section_number': section_num,
                            'line_start': line_num - len(current_section),
                            'content': '\n'.join(current_section)
                        })
                        section_num += 1
                        current_section = []
            
            # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
            if current_section:
                sections.append({
                    'section_number': section_num,
                    'line_start': len(lines) - len(current_section) + 1,
                    'content': '\n'.join(current_section)
                })
            
            return {
                'content': cleaned_content,
                'metadata': {
                    'line_count': len(lines),
                    'section_count': len(sections),
                    'word_count': len(cleaned_content.split())
                },
                'sections': sections,
                'file_type': 'txt',
                'file_size': file_path.stat().st_size
            }
            
        except Exception as e:
            self.log_error(f"è§£ææ–‡æœ¬æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}", error=e)
            raise
    
    def _clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¸…ç†åçš„æ–‡æœ¬
        """
        
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()\[\]{}\'"-]', ' ', text)
        
        # æ ‡å‡†åŒ–ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        æå–æ–‡ä»¶å…ƒæ•°æ®
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: æ–‡ä»¶å…ƒæ•°æ®
        """
        
        try:
            stat = file_path.stat()
            
            metadata = {
                'file_name': file_path.name,
                'file_size': stat.st_size,
                'file_type': file_path.suffix.lower().lstrip('.'),
                'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'absolute_path': str(file_path.absolute())
            }
            
            return metadata
            
        except Exception as e:
            self.log_error(f"æå–æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}", error=e)
            return {}

# åˆ›å»ºå…¨å±€æ–‡æ¡£è§£æå™¨å®ä¾‹
document_parser = DocumentParser()
```

#### 4. åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
**æ–‡ä»¶ï¼š`app/services/processing/text_splitter.py`**

```python
"""
æ–‡æœ¬åˆ†å‰²æœåŠ¡
å°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºé€‚åˆå¤„ç†çš„å—
"""

import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from app.utils.logger import LoggerMixin

@dataclass
class TextChunk:
    """æ–‡æœ¬å—æ•°æ®ç±»"""
    content: str
    metadata: Dict[str, Any]
    chunk_index: int
    total_chunks: int

class TextSplitter(LoggerMixin):
    """
    æ–‡æœ¬åˆ†å‰²å™¨
    å°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºé€‚åˆå‘é‡åŒ–çš„å—
    """
    
    def __init__(
        self, 
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        separators: Optional[List[str]] = None
    ):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        
        Args:
            chunk_size: æ¯ä¸ªå—çš„å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: å—ä¹‹é—´çš„é‡å å¤§å°
            separators: åˆ†éš”ç¬¦åˆ—è¡¨
        """
        super().__init__()
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        if separators is None:
            separators = [
                "\n\n",  # åŒæ¢è¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰
                "\n",    # å•æ¢è¡Œ
                "ã€‚",    # ä¸­æ–‡å¥å·
                "ï¼",    # ä¸­æ–‡æ„Ÿå¹å·
                "ï¼Ÿ",    # ä¸­æ–‡é—®å·
                "ï¼›",    # ä¸­æ–‡åˆ†å·
                "ï¼Œ",    # ä¸­æ–‡é€—å·
                ". ",    # è‹±æ–‡å¥å·+ç©ºæ ¼
                "! ",    # è‹±æ–‡æ„Ÿå¹å·+ç©ºæ ¼
                "? ",    # è‹±æ–‡é—®å·+ç©ºæ ¼
                "; ",    # è‹±æ–‡åˆ†å·+ç©ºæ ¼
                ", ",    # è‹±æ–‡é€—å·+ç©ºæ ¼
                " ",     # ç©ºæ ¼
                ""       # æœ€åæŒ‰å­—ç¬¦åˆ†å‰²
            ]
        
        self.separators = separators
        
        self.log_info(f"æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ - å—å¤§å°: {chunk_size}, é‡å : {chunk_overlap}")
    
    def split_text(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[TextChunk]:
        """
        åˆ†å‰²æ–‡æœ¬
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            metadata: åŸå§‹å…ƒæ•°æ®
            
        Returns:
            List[TextChunk]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        
        if metadata is None:
            metadata = {}
        
        self.log_debug(f"å¼€å§‹åˆ†å‰²æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
        
        try:
            # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
            if len(text) <= self.chunk_size:
                return [TextChunk(
                    content=text,
                    metadata=metadata,
                    chunk_index=0,
                    total_chunks=1
                )]
            
            # å°è¯•ç”¨ä¸åŒçš„åˆ†éš”ç¬¦åˆ†å‰²
            chunks = self._split_by_separators(text)
            
            # å¦‚æœåˆ†å‰²åçš„å—ä»ç„¶å¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            final_chunks = []
            for i, chunk_text in enumerate(chunks):
                if len(chunk_text) > self.chunk_size:
                    # é€’å½’åˆ†å‰²
                    sub_chunks = self._recursive_split(chunk_text)
                    for j, sub_chunk in enumerate(sub_chunks):
                        chunk_metadata = metadata.copy()
                        chunk_metadata['original_chunk_index'] = i
                        chunk_metadata['sub_chunk_index'] = j
                        
                        final_chunks.append(TextChunk(
                            content=sub_chunk,
                            metadata=chunk_metadata,
                            chunk_index=len(final_chunks),
                            total_chunks=0  # ç¨åæ›´æ–°
                        ))
                else:
                    chunk_metadata = metadata.copy()
                    chunk_metadata['original_chunk_index'] = i
                    
                    final_chunks.append(TextChunk(
                        content=chunk_text,
                        metadata=chunk_metadata,
                        chunk_index=len(final_chunks),
                        total_chunks=0  # ç¨åæ›´æ–°
                    ))
            
            # æ›´æ–°æ€»æ•°
            for i, chunk in enumerate(final_chunks):
                chunk.total_chunks = len(final_chunks)
                # æ›´æ–°ç´¢å¼•ä»¥åæ˜ æœ€ç»ˆä½ç½®
                chunk.chunk_index = i
            
            self.log_debug(f"æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(final_chunks)} ä¸ªå—")
            
            return final_chunks
            
        except Exception as e:
            self.log_error(f"åˆ†å‰²æ–‡æœ¬å¤±è´¥: {str(e)}", error=e)
            # å¤±è´¥æ—¶è¿”å›æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªå—
            return [TextChunk(
                content=text,
                metadata=metadata,
                chunk_index=0,
                total_chunks=1
            )]
    
    def _split_by_separators(self, text: str) -> List[str]:
        """ä½¿ç”¨åˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬"""
        
        chunks = [text]
        
        for separator in self.separators:
            if separator == "":
                # æœ€åçš„åˆ†éš”ç¬¦ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²
                final_chunks = []
                for chunk in chunks:
                    if len(chunk) > self.chunk_size:
                        # æŒ‰å­—ç¬¦åˆ†å‰²
                        char_chunks = self._split_by_characters(chunk)
                        final_chunks.extend(char_chunks)
                    else:
                        final_chunks.append(chunk)
                return final_chunks
            
            new_chunks = []
            for chunk in chunks:
                if len(chunk) <= self.chunk_size:
                    new_chunks.append(chunk)
                else:
                    # ä½¿ç”¨å½“å‰åˆ†éš”ç¬¦åˆ†å‰²
                    split_chunks = self._split_by_separator(chunk, separator)
                    new_chunks.extend(split_chunks)
            
            chunks = new_chunks
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å—éƒ½å·²è¶³å¤Ÿå°
            if all(len(chunk) <= self.chunk_size for chunk in chunks):
                return chunks
        
        return chunks
    
    def _split_by_separator(self, text: str, separator: str) -> List[str]:
        """ä½¿ç”¨ç‰¹å®šåˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬"""
        
        if separator == "":
            return [text]
        
        # åˆ†å‰²æ–‡æœ¬
        parts = text.split(separator)
        
        # é‡æ–°æ·»åŠ åˆ†éš”ç¬¦ï¼ˆé™¤äº†æœ€åä¸€éƒ¨åˆ†ï¼‰
        result = []
        for i, part in enumerate(parts):
            if i < len(parts) - 1:
                result.append(part + separator)
            else:
                result.append(part)
        
        # åˆå¹¶å°ç‰‡æ®µ
        merged_result = []
        current_chunk = ""
        
        for part in result:
            if len(current_chunk) + len(part) <= self.chunk_size:
                current_chunk += part
            else:
                if current_chunk:
                    merged_result.append(current_chunk)
                # å¦‚æœå•ä¸ªéƒ¨åˆ†å°±è¶…è¿‡å—å¤§å°ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                if len(part) > self.chunk_size:
                    # é€’å½’åˆ†å‰²
                    sub_parts = self._recursive_split(part)
                    merged_result.extend(sub_parts)
                    current_chunk = ""
                else:
                    current_chunk = part
        
        if current_chunk:
            merged_result.append(current_chunk)
        
        return merged_result
    
    def _split_by_characters(self, text: str) -> List[str]:
        """æŒ‰å­—ç¬¦åˆ†å‰²æ–‡æœ¬"""
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            
            # å°è¯•åœ¨å¥å­è¾¹ç•Œå¤„ç»“æŸ
            if end < len(text):
                # æŸ¥æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸç¬¦
                sentence_end = max(
                    text.rfind('ã€‚', start, end),
                    text.rfind('ï¼', start, end),
                    text.rfind('ï¼Ÿ', start, end),
                    text.rfind('. ', start, end),
                    text.rfind('! ', start, end),
                    text.rfind('? ', start, end),
                    text.rfind('\n', start, end)
                )
                
                if sentence_end != -1 and sentence_end > start + self.chunk_size // 2:
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # æ›´æ–°èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            start = end - self.chunk_overlap if end - self.chunk_overlap > start else end
        
        return chunks
    
    def _recursive_split(self, text: str) -> List[str]:
        """é€’å½’åˆ†å‰²æ–‡æœ¬"""
        
        if len(text) <= self.chunk_size:
            return [text]
        
        # åœ¨ä¸­é—´ä½ç½®æŸ¥æ‰¾åˆ†å‰²ç‚¹
        mid = len(text) // 2
        
        # æŸ¥æ‰¾é™„è¿‘çš„åˆ†éš”ç¬¦
        best_split = -1
        for offset in range(0, min(100, len(text) - mid)):
            # å‘å‰æŸ¥æ‰¾
            forward_pos = mid + offset
            if forward_pos < len(text):
                if self._is_good_split_point(text, forward_pos):
                    best_split = forward_pos
                    break
            
            # å‘åæŸ¥æ‰¾
            backward_pos = mid - offset
            if backward_pos > 0:
                if self._is_good_split_point(text, backward_pos):
                    best_split = backward_pos
                    break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„åˆ†å‰²ç‚¹ï¼Œåœ¨ä¸­é—´åˆ†å‰²
        if best_split == -1:
            best_split = mid
        
        # é€’å½’åˆ†å‰²
        left_part = text[:best_split].strip()
        right_part = text[best_split:].strip()
        
        left_chunks = self._recursive_split(left_part) if left_part else []
        right_chunks = self._recursive_split(right_part) if right_part else []
        
        return left_chunks + right_chunks
    
    def _is_good_split_point(self, text: str, position: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å¥½çš„åˆ†å‰²ç‚¹"""
        
        if position <= 0 or position >= len(text):
            return False
        
        # æ£€æŸ¥ä½ç½®å‰åå­—ç¬¦
        prev_char = text[position - 1] if position > 0 else ''
        curr_char = text[position] if position < len(text) else ''
        
        # å¥½çš„åˆ†å‰²ç‚¹ï¼šå¥å­ç»“æŸç¬¦å
        if prev_char in 'ã€‚ï¼ï¼Ÿ.!?':
            return True
        
        # æ®µè½åˆ†éš”
        if position >= 2 and text[position-2:position] == '\n\n':
            return True
        
        # åœ¨ç©ºæ ¼å¤„åˆ†éš”
        if prev_char == ' ' and curr_char != ' ':
            return True
        
        return False
    
    def split_documents(self, documents: List[Dict[str, Any]]) -> List[TextChunk]:
        """
        åˆ†å‰²å¤šä¸ªæ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«contentå’Œmetadata
            
        Returns:
            List[TextChunk]: æ‰€æœ‰æ–‡æ¡£çš„åˆ†å‰²ç»“æœ
        """
        
        all_chunks = []
        
        for doc_idx, doc in enumerate(documents):
            content = doc.get('content', '')
            metadata = doc.get('metadata', {}).copy()
            
            # æ·»åŠ æ–‡æ¡£ä¿¡æ¯åˆ°å…ƒæ•°æ®
            metadata['document_index'] = doc_idx
            metadata['document_source'] = doc.get('source', 'unknown')
            
            chunks = self.split_text(content, metadata)
            all_chunks.extend(chunks)
        
        # æ›´æ–°æ€»æ•°å’Œç´¢å¼•
        for i, chunk in enumerate(all_chunks):
            chunk.chunk_index = i
            chunk.total_chunks = len(all_chunks)
        
        self.log_info(f"åˆ†å‰² {len(documents)} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        return all_chunks

# åˆ›å»ºå…¨å±€æ–‡æœ¬åˆ†å‰²å™¨å®ä¾‹
text_splitter = TextSplitter()
```

### ğŸ¯ ç¬¬ä¸‰é˜¶æ®µï¼šå‘é‡åŒ–ä¸å­˜å‚¨ï¼ˆ2-3å¤©ï¼‰

#### 5. åˆ›å»ºåµŒå…¥æœåŠ¡
**æ–‡ä»¶ï¼š`app/services/processing/embedding_service.py`**

```python
"""
åµŒå…¥æœåŠ¡
ç”Ÿæˆæ–‡æœ¬çš„å‘é‡åµŒå…¥
"""

import numpy as np
from typing import List, Dict, Any, Optional, Union
from sentence_transformers import SentenceTransformer

from app.config.settings import settings
from app.utils.logger import LoggerMixin

class EmbeddingService(LoggerMixin):
    """
    åµŒå…¥æœåŠ¡
    ä½¿ç”¨Sentence Transformersç”Ÿæˆæ–‡æœ¬å‘é‡
    """
    
    def __init__(self, model_name: Optional[str] = None):
        """
        åˆå§‹åŒ–åµŒå…¥æœåŠ¡
        
        Args:
            model_name: åµŒå…¥æ¨¡å‹åç§°
        """
        super().__init__()
        
        self.model_name = model_name or settings.EMBEDDING_MODEL
        
        try:
            self.log_info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            
            # è·å–æ¨¡å‹ç»´åº¦
            self.dimension = self.model.get_sentence_embedding_dimension()
            
            self.log_info(f"åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ - ç»´åº¦: {self.dimension}")
            
        except Exception as e:
            self.log_error(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {self.model_name}, é”™è¯¯: {str(e)}", error=e)
            raise
    
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            **kwargs: é¢å¤–çš„ç¼–ç å‚æ•°
            
        Returns:
            np.ndarray: å‘é‡æ•°ç»„
        """
        
        try:
            if isinstance(texts, str):
                texts = [texts]
            
            self.log_debug(f"ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬ï¼Œæ¨¡å‹: {self.model_name}")
            
            # ç¼–ç æ–‡æœ¬
            embeddings = self.model.encode(
                texts,
                convert_to_numpy=True,
                show_progress_bar=False,
                **kwargs
            )
            
            return embeddings
            
        except Exception as e:
            self.log_error(f"ç¼–ç æ–‡æœ¬å¤±è´¥: {str(e)}", error=e)
            raise
    
    def encode_batch(self, texts: List[str], batch_size: int = 32, **kwargs) -> List[np.ndarray]:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            **kwargs: é¢å¤–çš„ç¼–ç å‚æ•°
            
        Returns:
            List[np.ndarray]: å‘é‡åˆ—è¡¨
        """
        
        try:
            self.log_debug(f"æ‰¹é‡ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
            
            all_embeddings = []
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                batch_embeddings = self.encode(batch_texts, **kwargs)
                all_embeddings.extend(batch_embeddings)
                
                self.log_debug(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts)+batch_size-1)//batch_size}")
            
            return all_embeddings
            
        except Exception as e:
            self.log_error(f"æ‰¹é‡ç¼–ç å¤±è´¥: {str(e)}", error=e)
            raise
    
    def similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        Args:
            embedding1: ç¬¬ä¸€ä¸ªå‘é‡
            embedding2: ç¬¬äºŒä¸ªå‘é‡
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        
        try:
            # å½’ä¸€åŒ–å‘é‡
            embedding1_norm = embedding1 / np.linalg.norm(embedding1)
            embedding2_norm = embedding2 / np.linalg.norm(embedding2)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(embedding1_norm, embedding2_norm)
            
            # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
            similarity = max(0.0, min(1.0, similarity))
            
            return similarity
            
        except Exception as e:
            self.log_error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}", error=e)
            return 0.0
    
    def similarity_batch(self, query_embedding: np.ndarray, embeddings: List[np.ndarray]) -> List[float]:
        """
        æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            embeddings: ç›®æ ‡å‘é‡åˆ—è¡¨
            
        Returns:
            List[float]: ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨
        """
        
        try:
            # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
            query_norm = query_embedding / np.linalg.norm(query_embedding)
            
            similarities = []
            
            for emb in embeddings:
                # å½’ä¸€åŒ–ç›®æ ‡å‘é‡
                emb_norm = emb / np.linalg.norm(emb)
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                sim = np.dot(query_norm, emb_norm)
                sim = max(0.0, min(1.0, sim))
                
                similarities.append(sim)
            
            return similarities
            
        except Exception as e:
            self.log_error(f"æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}", error=e)
            return [0.0] * len(embeddings)
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        
        return {
            'model_name': self.model_name,
            'dimension': self.dimension,
            'max_seq_length': self.model.max_seq_length,
            'device': str(self.model.device)
        }

# åˆ›å»ºå…¨å±€åµŒå…¥æœåŠ¡å®ä¾‹
embedding_service = EmbeddingService()
```

#### 6. åˆ›å»ºå‘é‡å­˜å‚¨
**æ–‡ä»¶ï¼š`app/services/processing/vector_store.py`**

```python
"""
å‘é‡å­˜å‚¨æœåŠ¡
ä½¿ç”¨ChromaDBå­˜å‚¨å’Œæ£€ç´¢å‘é‡
"""

import chromadb
import uuid
import json
from typing import List, Dict, Any, Optional, Tuple
from chromadb.config import Settings

from app.config.settings import settings
from app.utils.logger import LoggerMixin

class VectorStore(LoggerMixin):
    """
    å‘é‡å­˜å‚¨ç®¡ç†å™¨
    åŸºäºChromaDBçš„å‘é‡å­˜å‚¨å’Œæ£€ç´¢
    """
    
    def __init__(self, persist_directory: Optional[str] = None):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨
        
        Args:
            persist_directory: æŒä¹…åŒ–ç›®å½•
        """
        super().__init__()
        
        self.persist_directory = persist_directory or settings.CHROMA_PERSIST_DIR
        self.collection_name = settings.CHROMA_COLLECTION_NAME
        
        try:
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
            self.client = chromadb.PersistentClient(
                path=self.persist_directory,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            self.log_info(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆ - ç›®å½•: {self.persist_directory}")
            
            # åˆ›å»ºæˆ–è·å–é»˜è®¤é›†åˆ
            self.collection = self._get_or_create_collection()
            
            self.log_info(f"é›†åˆ '{self.collection_name}' å°±ç»ª")
            
        except Exception as e:
            self.log_error(f"åˆå§‹åŒ–å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}", error=e)
            raise
    
    def _get_or_create_collection(self) -> chromadb.Collection:
        """
        è·å–æˆ–åˆ›å»ºé›†åˆ
        
        Returns:
            chromadb.Collection: é›†åˆå¯¹è±¡
        """
        
        try:
            # å°è¯•è·å–ç°æœ‰é›†åˆ
            collection = self.client.get_collection(self.collection_name)
            self.log_debug(f"è·å–ç°æœ‰é›†åˆ: {self.collection_name}")
            return collection
            
        except Exception:
            # åˆ›å»ºæ–°é›†åˆ
            self.log_info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}")
            return self.client.create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            )
    
    def add_documents(
        self, 
        documents: List[str],
        embeddings: List[List[float]],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None
    ) -> List[str]:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
        
        Args:
            documents: æ–‡æ¡£å†…å®¹åˆ—è¡¨
            embeddings: å‘é‡åˆ—è¡¨
            metadatas: å…ƒæ•°æ®åˆ—è¡¨
            ids: IDåˆ—è¡¨
            
        Returns:
            List[str]: æ·»åŠ çš„æ–‡æ¡£IDåˆ—è¡¨
        """
        
        try:
            if not documents or not embeddings:
                self.log_warning("æ–‡æ¡£æˆ–å‘é‡ä¸ºç©º")
                return []
            
            if len(documents) != len(embeddings):
                raise ValueError(f"æ–‡æ¡£æ•°é‡({len(documents)})å’Œå‘é‡æ•°é‡({len(embeddings)})ä¸åŒ¹é…")
            
            # ç”ŸæˆID
            if ids is None:
                ids = [str(uuid.uuid4()) for _ in range(len(documents))]
            
            # å‡†å¤‡å…ƒæ•°æ®
            if metadatas is None:
                metadatas = [{} for _ in range(len(documents))]
            
            # æ·»åŠ æ–‡æ¡£
            self.collection.add(
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            self.log_info(f"æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡å­˜å‚¨")
            
            return ids
            
        except Exception as e:
            self.log_error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}", error=e)
            raise
    
    def search(
        self,
        query_embedding: List[float],
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None,
        where_document: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            n_results: è¿”å›ç»“æœæ•°é‡
            where: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            where_document: æ–‡æ¡£å†…å®¹è¿‡æ»¤æ¡ä»¶
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        
        try:
            self.log_debug(f"å‘é‡æœç´¢ - æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_embedding)}, è¿”å›æ•°é‡: {n_results}")
            
            # æ‰§è¡Œæœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where,
                where_document=where_document,
                include=["documents", "metadatas", "distances"]
            )
            
            # æ•´ç†ç»“æœ
            search_results = []
            
            if results['documents'] and results['documents'][0]:
                for i in range(len(results['documents'][0])):
                    # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆChromaDBè¿”å›çš„æ˜¯è·ç¦»ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
                    distance = results['distances'][0][i]
                    similarity = 1.0 - distance  # å‡è®¾ä½¿ç”¨ä½™å¼¦è·ç¦»
                    
                    result = {
                        'id': results['ids'][0][i],
                        'content': results['documents'][0][i],
                        'score': similarity,
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                        'distance': distance
                    }
                    
                    search_results.append(result)
            
            self.log_debug(f"æœç´¢å®Œæˆï¼Œè¿”å› {len(search_results)} ä¸ªç»“æœ")
            
            return search_results
            
        except Exception as e:
            self.log_error(f"æœç´¢å¤±è´¥: {str(e)}", error=e)
            return []
    
    def search_by_text(
        self,
        query_text: str,
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        é€šè¿‡æ–‡æœ¬æœç´¢
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            where: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        
        try:
            self.log_debug(f"æ–‡æœ¬æœç´¢ - æŸ¥è¯¢: '{query_text[:50]}...', è¿”å›æ•°é‡: {n_results}")
            
            # ä½¿ç”¨ChromaDBçš„æ–‡æœ¬æœç´¢
            results = self.collection.query(
                query_texts=[query_text],
                n_results=n_results,
                where=where,
                include=["documents", "metadatas", "distances"]
            )
            
            # æ•´ç†ç»“æœ
            search_results = []
            
            if results['documents'] and results['documents'][0]:
                for i in range(len(results['documents'][0])):
                    distance = results['distances'][0][i]
                    similarity = 1.0 - distance
                    
                    result = {
                        'id': results['ids'][0][i],
                        'content': results['documents'][0][i],
                        'score': similarity,
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                        'distance': distance
                    }
                    
                    search_results.append(result)
            
            self.log_debug(f"æ–‡æœ¬æœç´¢å®Œæˆï¼Œè¿”å› {len(search_results)} ä¸ªç»“æœ")
            
            return search_results
            
        except Exception as e:
            self.log_error(f"æ–‡æœ¬æœç´¢å¤±è´¥: {str(e)}", error=e)
            return []
    
    def get_document(self, document_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å•ä¸ªæ–‡æ¡£
        
        Args:
            document_id: æ–‡æ¡£ID
            
        Returns:
            Optional[Dict]: æ–‡æ¡£ä¿¡æ¯
        """
        
        try:
            results = self.collection.get(
                ids=[document_id],
                include=["documents", "metadatas", "embeddings"]
            )
            
            if results['ids']:
                return {
                    'id': results['ids'][0],
                    'content': results['documents'][0],
                    'metadata': results['metadatas'][0] if results['metadatas'] else {},
                    'embedding': results['embeddings'][0] if results['embeddings'] else None
                }
            
            return None
            
        except Exception as e:
            self.log_error(f"è·å–æ–‡æ¡£å¤±è´¥: {document_id}, é”™è¯¯: {str(e)}", error=e)
            return None
    
    def delete_documents(self, document_ids: List[str]) -> bool:
        """
        åˆ é™¤æ–‡æ¡£
        
        Args:
            document_ids: æ–‡æ¡£IDåˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        
        try:
            self.collection.delete(ids=document_ids)
            self.log_info(f"åˆ é™¤ {len(document_ids)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            self.log_error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}", error=e)
            return False
    
    def delete_by_filter(self, where: Dict[str, Any]) -> bool:
        """
        æ ¹æ®è¿‡æ»¤æ¡ä»¶åˆ é™¤æ–‡æ¡£
        
        Args:
            where: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        
        try:
            self.collection.delete(where=where)
            self.log_info(f"æ ¹æ®è¿‡æ»¤æ¡ä»¶åˆ é™¤æ–‡æ¡£: {where}")
            return True
            
        except Exception as e:
            self.log_error(f"æ ¹æ®è¿‡æ»¤æ¡ä»¶åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}", error=e)
            return False
    
    def get_collection_info(self) -> Dict[str, Any]:
        """
        è·å–é›†åˆä¿¡æ¯
        
        Returns:
            Dict: é›†åˆä¿¡æ¯
        """
        
        try:
            count = self.collection.count()
            
            return {
                'collection_name': self.collection_name,
                'document_count': count,
                'persist_directory': self.persist_directory
            }
            
        except Exception as e:
            self.log_error(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}", error=e)
            return {}
    
    def reset_collection(self) -> bool:
        """
        é‡ç½®é›†åˆï¼ˆåˆ é™¤æ‰€æœ‰æ•°æ®ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸé‡ç½®
        """
        
        try:
            self.client.delete_collection(self.collection_name)
            self.collection = self._get_or_create_collection()
            
            self.log_info("é›†åˆé‡ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            self.log_error(f"é‡ç½®é›†åˆå¤±è´¥: {str(e)}", error=e)
            return False

# åˆ›å»ºå…¨å±€å‘é‡å­˜å‚¨å®ä¾‹
vector_store = VectorStore()
```

### ğŸ¯ ç¬¬å››é˜¶æ®µï¼šRAGæœåŠ¡ä¸APIï¼ˆ2-3å¤©ï¼‰

#### 7. åˆ›å»ºRAGæœåŠ¡
**æ–‡ä»¶ï¼š`app/services/ai/rag_service.py`**

```python
"""
RAGæ£€ç´¢æœåŠ¡
æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ ¸å¿ƒæœåŠ¡
"""

from typing import List, Dict, Any, Optional, Tuple
import numpy as np

from app.services.processing.embedding_service import embedding_service
from app.services.processing.vector_store import vector_store
from app.services.knowledge_service import knowledge_service
from app.utils.logger import LoggerMixin

class RAGService(LoggerMixin):
    """
    RAGæ£€ç´¢æœåŠ¡
    å¤„ç†çŸ¥è¯†æ£€ç´¢å’Œä¸Šä¸‹æ–‡æ„å»º
    """
    
    def __init__(self):
        """åˆå§‹åŒ–RAGæœåŠ¡"""
        super().__init__()
        self.log_info("RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def retrieve(
        self, 
        query: str, 
        knowledge_base_id: str, 
        top_k: int = 5,
        score_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸å…³çŸ¥è¯†
        
        Args:
            query: æŸ¥è¯¢å†…å®¹
            knowledge_base_id: çŸ¥è¯†åº“ID
            top_k: è¿”å›ç»“æœæ•°é‡
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List[Dict]: æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        
        try:
            self.log_info(f"RAGæ£€ç´¢ - çŸ¥è¯†åº“: {knowledge_base_id}, æŸ¥è¯¢: '{query[:50]}...'")
            
            # 1. ç¼–ç æŸ¥è¯¢
            query_embedding = embedding_service.encode(query)
            
            # 2. æ„å»ºè¿‡æ»¤æ¡ä»¶ï¼ˆæŒ‰çŸ¥è¯†åº“IDè¿‡æ»¤ï¼‰
            where_filter = {"knowledge_base_id": knowledge_base_id}
            
            # 3. å‘é‡æœç´¢
            vector_results = vector_store.search(
                query_embedding=query_embedding.tolist(),
                n_results=top_k * 2,  # å¤šå–ä¸€äº›ï¼Œæ–¹ä¾¿åç»­è¿‡æ»¤
                where=where_filter
            )
            
            # 4. æ–‡æœ¬æœç´¢ï¼ˆä½œä¸ºè¡¥å……ï¼‰
            text_results = vector_store.search_by_text(
                query_text=query,
                n_results=top_k,
                where=where_filter
            )
            
            # 5. åˆå¹¶å’Œå»é‡ç»“æœ
            all_results = self._merge_results(vector_results, text_results)
            
            # 6. è¿‡æ»¤ä½åˆ†ç»“æœ
            filtered_results = [
                result for result in all_results 
                if result.get('score', 0) >= score_threshold
            ]
            
            # 7. æ’åºå’Œæˆªæ–­
            filtered_results.sort(key=lambda x: x.get('score', 0), reverse=True)
            final_results = filtered_results[:top_k]
            
            self.log_info(f"RAGæ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            
            return final_results
            
        except Exception as e:
            self.log_error(f"RAGæ£€ç´¢å¤±è´¥: {str(e)}", error=e)
            return []
    
    def _merge_results(self, vector_results: List[Dict], text_results: List[Dict]) -> List[Dict]:
        """
        åˆå¹¶å‘é‡æœç´¢å’Œæ–‡æœ¬æœç´¢ç»“æœ
        
        Args:
            vector_results: å‘é‡æœç´¢ç»“æœ
            text_results: æ–‡æœ¬æœç´¢ç»“æœ
            
        Returns:
            List[Dict]: åˆå¹¶åçš„ç»“æœ
        """
        
        merged = []
        seen_ids = set()
        
        # æ·»åŠ å‘é‡æœç´¢ç»“æœ
        for result in vector_results:
            result_id = result.get('id')
            if result_id and result_id not in seen_ids:
                merged.append(result)
                seen_ids.add(result_id)
        
        # æ·»åŠ æ–‡æœ¬æœç´¢ç»“æœï¼ˆå»é‡ï¼‰
        for result in text_results:
            result_id = result.get('id')
            if result_id and result_id not in seen_ids:
                merged.append(result)
                seen_ids.add(result_id)
        
        return merged
    
    def build_context(self, query: str, search_results: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            search_results: æ£€ç´¢ç»“æœ
            
        Returns:
            str: æ„å»ºçš„ä¸Šä¸‹æ–‡
        """
        
        if not search_results:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        try:
            context_parts = []
            context_parts.append(f"ç”¨æˆ·æŸ¥è¯¢: {query}\n\n")
            context_parts.append("ç›¸å…³æ–‡æ¡£å†…å®¹:\n")
            
            for i, result in enumerate(search_results, 1):
                content = result.get('content', '')
                metadata = result.get('metadata', {})
                score = result.get('score', 0)
                
                # æ ¼å¼åŒ–å…ƒæ•°æ®
                metadata_str = ""
                if metadata:
                    metadata_items = []
                    for key, value in metadata.items():
                        if key not in ['knowledge_base_id', 'embedding']:
                            metadata_items.append(f"{key}: {value}")
                    if metadata_items:
                        metadata_str = f" ({', '.join(metadata_items)})"
                
                context_parts.append(f"\n--- æ–‡æ¡£ {i} (ç›¸å…³åº¦: {score:.2f}){metadata_str} ---\n")
                context_parts.append(f"{content}\n")
            
            # æ·»åŠ å¼•ç”¨è¯´æ˜
            context_parts.append("\n--- è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ ---")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            self.log_error(f"æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}", error=e)
            return "ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
    
    def answer_with_rag(
        self, 
        query: str, 
        knowledge_base_id: str,
        conversation_context: Optional[str] = None,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨RAGå›ç­”é—®é¢˜
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            knowledge_base_id: çŸ¥è¯†åº“ID
            conversation_context: å¯¹è¯ä¸Šä¸‹æ–‡
            top_k: æ£€ç´¢æ•°é‡
            
        Returns:
            Dict: å›ç­”ç»“æœ
        """
        
        try:
            self.log_info(f"RAGé—®ç­” - çŸ¥è¯†åº“: {knowledge_base_id}, é—®é¢˜: '{query[:50]}...'")
            
            # 1. æ£€ç´¢ç›¸å…³çŸ¥è¯†
            search_results = self.retrieve(
                query=query, 
                knowledge_base_id=knowledge_base_id,
                top_k=top_k
            )
            
            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context = self.build_context(query, search_results)
            
            # 3. å¦‚æœæœ‰å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œåˆå¹¶
            if conversation_context:
                full_context = f"å¯¹è¯å†å²:\n{conversation_context}\n\n{context}"
            else:
                full_context = context
            
            # 4. æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(query, full_context)
            
            # 5. è¿”å›ç»“æœ
            return {
                'success': True,
                'query': query,
                'context': full_context,
                'prompt': prompt,
                'search_results': search_results,
                'result_count': len(search_results)
            }
            
        except Exception as e:
            self.log_error(f"RAGé—®ç­”å¤±è´¥: {str(e)}", error=e)
            return {
                'success': False,
                'error': str(e),
                'query': query,
                'context': "æ£€ç´¢å¤±è´¥",
                'search_results': []
            }
    
    def _build_prompt(self, query: str, context: str) -> str:
        """
        æ„å»ºæç¤ºè¯
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            str: æç¤ºè¯
        """
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æä¾›çš„ç›¸å…³æ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{context}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
1. åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°å‘ŠçŸ¥ç”¨æˆ·
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰å¸®åŠ©
4. å¯ä»¥å¼•ç”¨æ–‡æ¡£ä¸­çš„å…·ä½“ä¿¡æ¯ï¼Œä½†ä¸è¦ç›´æ¥å¤åˆ¶å¤§æ®µåŸæ–‡
5. ä½¿ç”¨ä¸­æ–‡å›ç­”

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·å¼€å§‹å›ç­”ï¼š"""
        
        return prompt
    
    def add_documents_to_knowledge_base(
        self, 
        knowledge_base_id: str,
        documents: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        å°†æ–‡æ¡£æ·»åŠ åˆ°çŸ¥è¯†åº“å¹¶å‘é‡åŒ–
        
        Args:
            knowledge_base_id: çŸ¥è¯†åº“ID
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        
        try:
            if not documents:
                return {
                    'success': False,
                    'error': 'æ–‡æ¡£åˆ—è¡¨ä¸ºç©º',
                    'processed_count': 0
                }
            
            self.log_info(f"å¼€å§‹å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“ {knowledge_base_id}")
            
            # 1. å‡†å¤‡æ–‡æ¡£å†…å®¹
            texts = []
            metadatas = []
            
            for doc in documents:
                content = doc.get('content', '')
                metadata = doc.get('metadata', {}).copy()
                
                # æ·»åŠ çŸ¥è¯†åº“IDåˆ°å…ƒæ•°æ®
                metadata['knowledge_base_id'] = knowledge_base_id
                
                texts.append(content)
                metadatas.append(metadata)
            
            # 2. ç”Ÿæˆå‘é‡
            embeddings = embedding_service.encode(texts)
            
            # 3. æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            document_ids = vector_store.add_documents(
                documents=texts,
                embeddings=embeddings.tolist(),
                metadatas=metadatas
            )
            
            # 4. æ›´æ–°çŸ¥è¯†åº“çŠ¶æ€
            knowledge_service.update_knowledge_base(
                knowledge_base_id,
                KnowledgeBaseUpdate(vectorized=True)
            )
            
            self.log_info(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼ŒæˆåŠŸæ·»åŠ  {len(document_ids)} ä¸ªæ–‡æ¡£")
            
            return {
                'success': True,
                'processed_count': len(document_ids),
                'document_ids': document_ids,
                'knowledge_base_id': knowledge_base_id
            }
            
        except Exception as e:
            self.log_error(f"æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“å¤±è´¥: {str(e)}", error=e)
            return {
                'success': False,
                'error': str(e),
                'processed_count': 0
            }

# åˆ›å»ºå…¨å±€RAGæœåŠ¡å®ä¾‹
rag_service = RAGService()
```

### ğŸ¯ ç¬¬äº”é˜¶æ®µï¼šAPIç«¯ç‚¹ä¸é›†æˆï¼ˆ1-2å¤©ï¼‰

#### 8. åˆ›å»ºçŸ¥è¯†åº“APIç«¯ç‚¹
**æ–‡ä»¶ï¼š`app/api/v1/endpoints/knowledge.py`**

```python
"""
çŸ¥è¯†åº“ç®¡ç†APIç«¯ç‚¹
"""

from typing import Optional, List
from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Form, BackgroundTasks

from app.api.dependencies import get_current_user, UserContext
from app.services.knowledge_service import knowledge_service
from app.services.processing.document_parser import document_parser
from app.services.processing.text_splitter import text_splitter
from app.services.ai.rag_service import rag_service
from app.models.schemas import (
    KnowledgeBaseCreate,
    KnowledgeBaseUpdate,
    KnowledgeBaseResponse,
    KnowledgeQuery,
    SearchResult,
    SuccessResponse,
    PaginationParams
)
from app.utils.logger import get_logger

logger = get_logger(__name__)

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/knowledge-bases", tags=["knowledge-bases"])

@router.get(
    "",
    response_model=SuccessResponse,
    summary="è·å–çŸ¥è¯†åº“åˆ—è¡¨",
    description="è·å–çŸ¥è¯†åº“åˆ—è¡¨ï¼Œæ”¯æŒåˆ†é¡µå’Œè¿‡æ»¤"
)
async def get_knowledge_bases(
    status: Optional[str] = None,
    is_public: Optional[bool] = None,
    page: int = 1,
    page_size: int = 20,
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    è·å–çŸ¥è¯†åº“åˆ—è¡¨ç«¯ç‚¹
    
    Args:
        status: çŠ¶æ€è¿‡æ»¤
        is_public: æ˜¯å¦å…¬å¼€è¿‡æ»¤
        page: é¡µç 
        page_size: æ¯é¡µå¤§å°
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”ï¼ŒåŒ…å«çŸ¥è¯†åº“åˆ—è¡¨
    """
    
    try:
        # è®¡ç®—åç§»é‡
        offset = (page - 1) * page_size
        
        # è·å–çŸ¥è¯†åº“åˆ—è¡¨
        knowledge_bases = knowledge_service.list_knowledge_bases(
            user_id=current_user.user_id,
            status=status,
            is_public=is_public,
            limit=page_size,
            offset=offset
        )
        
        # è·å–æ€»æ•°
        total_kbs = len([
            kb for kb_id, kb_data in knowledge_service._knowledge_bases.items()
            if (not status or kb_data.get("status") == status) and
               (is_public is None or kb_data.get("is_public") == is_public) and
               (not current_user.user_id or kb_data.get("created_by") == current_user.user_id or kb_data.get("is_public"))
        ])
        
        # è®¡ç®—æ€»é¡µæ•°
        total_pages = (total_kbs + page_size - 1) // page_size
        
        logger.info(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨ - ç”¨æˆ·: {current_user.user_id}, æ•°é‡: {len(knowledge_bases)}")
        
        return SuccessResponse(
            success=True,
            message="è·å–çŸ¥è¯†åº“åˆ—è¡¨æˆåŠŸ",
            data={
                "items": [kb.dict() for kb in knowledge_bases],
                "total": total_kbs,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
                "has_next": page < total_pages,
                "has_prev": page > 1
            }
        )
        
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–çŸ¥è¯†åº“åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

@router.post(
    "",
    response_model=SuccessResponse,
    summary="åˆ›å»ºçŸ¥è¯†åº“",
    description="åˆ›å»ºæ–°çš„çŸ¥è¯†åº“"
)
async def create_knowledge_base(
    kb_data: KnowledgeBaseCreate,
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    åˆ›å»ºçŸ¥è¯†åº“ç«¯ç‚¹
    
    Args:
        kb_data: çŸ¥è¯†åº“æ•°æ®
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”ï¼ŒåŒ…å«åˆ›å»ºçš„çŸ¥è¯†åº“ä¿¡æ¯
    """
    
    try:
        # åˆ›å»ºçŸ¥è¯†åº“
        knowledge_base = knowledge_service.create_knowledge_base(
            kb_data, 
            current_user.user_id
        )
        
        if not knowledge_base:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥"
            )
        
        logger.info(f"åˆ›å»ºçŸ¥è¯†åº“æˆåŠŸ - ID: {knowledge_base.id}, åç§°: {knowledge_base.name}")
        
        return SuccessResponse(
            success=True,
            message="çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ",
            data=knowledge_base.dict()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ›å»ºçŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

@router.get(
    "/{knowledge_base_id}",
    response_model=SuccessResponse,
    summary="è·å–çŸ¥è¯†åº“è¯¦æƒ…",
    description="è·å–æŒ‡å®šçŸ¥è¯†åº“çš„è¯¦ç»†ä¿¡æ¯"
)
async def get_knowledge_base_detail(
    knowledge_base_id: str,
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    è·å–çŸ¥è¯†åº“è¯¦æƒ…ç«¯ç‚¹
    
    Args:
        knowledge_base_id: çŸ¥è¯†åº“ID
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”ï¼ŒåŒ…å«çŸ¥è¯†åº“è¯¦æƒ…
    """
    
    try:
        # è·å–çŸ¥è¯†åº“è¯¦æƒ…
        knowledge_base = knowledge_service.get_knowledge_base(knowledge_base_id)
        
        if not knowledge_base:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}"
            )
        
        # æ£€æŸ¥æƒé™ï¼ˆéå…¬å¼€çŸ¥è¯†åº“åªå…è®¸åˆ›å»ºè€…è®¿é—®ï¼‰
        if not knowledge_base.is_public and knowledge_base.created_by != current_user.user_id:
            logger.warning(f"æƒé™æ‹’ç» - ç”¨æˆ·: {current_user.user_id} å°è¯•è®¿é—®éå…¬å¼€çŸ¥è¯†åº“: {knowledge_base_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒè®¿é—®æ­¤çŸ¥è¯†åº“"
            )
        
        logger.info(f"è·å–çŸ¥è¯†åº“è¯¦æƒ… - ID: {knowledge_base_id}")
        
        return SuccessResponse(
            success=True,
            message="è·å–çŸ¥è¯†åº“è¯¦æƒ…æˆåŠŸ",
            data=knowledge_base.dict()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“è¯¦æƒ…å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–çŸ¥è¯†åº“è¯¦æƒ…æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

@router.put(
    "/{knowledge_base_id}",
    response_model=SuccessResponse,
    summary="æ›´æ–°çŸ¥è¯†åº“",
    description="æ›´æ–°æŒ‡å®šçŸ¥è¯†åº“çš„ä¿¡æ¯"
)
async def update_knowledge_base(
    knowledge_base_id: str,
    update_data: KnowledgeBaseUpdate,
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    æ›´æ–°çŸ¥è¯†åº“ç«¯ç‚¹
    
    Args:
        knowledge_base_id: çŸ¥è¯†åº“ID
        update_data: æ›´æ–°æ•°æ®
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”ï¼ŒåŒ…å«æ›´æ–°åçš„çŸ¥è¯†åº“ä¿¡æ¯
    """
    
    try:
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        existing_kb = knowledge_service.get_knowledge_base(knowledge_base_id)
        
        if not existing_kb:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}"
            )
        
        # æ£€æŸ¥æƒé™ï¼ˆä»…åˆ›å»ºè€…å¯ä»¥æ›´æ–°ï¼‰
        if existing_kb.created_by != current_user.user_id:
            logger.warning(f"æƒé™æ‹’ç» - ç”¨æˆ·: {current_user.user_id} å°è¯•æ›´æ–°çŸ¥è¯†åº“: {knowledge_base_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒæ›´æ–°æ­¤çŸ¥è¯†åº“"
            )
        
        # æ›´æ–°çŸ¥è¯†åº“
        updated_kb = knowledge_service.update_knowledge_base(knowledge_base_id, update_data)
        
        if not updated_kb:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="æ›´æ–°çŸ¥è¯†åº“å¤±è´¥"
            )
        
        logger.info(f"æ›´æ–°çŸ¥è¯†åº“æˆåŠŸ - ID: {knowledge_base_id}")
        
        return SuccessResponse(
            success=True,
            message="çŸ¥è¯†åº“æ›´æ–°æˆåŠŸ",
            data=updated_kb.dict()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°çŸ¥è¯†åº“å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ›´æ–°çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

@router.delete(
    "/{knowledge_base_id}",
    response_model=SuccessResponse,
    summary="åˆ é™¤çŸ¥è¯†åº“",
    description="åˆ é™¤æŒ‡å®šçš„çŸ¥è¯†åº“"
)
async def delete_knowledge_base(
    knowledge_base_id: str,
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    åˆ é™¤çŸ¥è¯†åº“ç«¯ç‚¹
    
    Args:
        knowledge_base_id: çŸ¥è¯†åº“ID
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”
    """
    
    try:
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        existing_kb = knowledge_service.get_knowledge_base(knowledge_base_id)
        
        if not existing_kb:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}"
            )
        
        # æ£€æŸ¥æƒé™ï¼ˆä»…åˆ›å»ºè€…å¯ä»¥åˆ é™¤ï¼‰
        if existing_kb.created_by != current_user.user_id:
            logger.warning(f"æƒé™æ‹’ç» - ç”¨æˆ·: {current_user.user_id} å°è¯•åˆ é™¤çŸ¥è¯†åº“: {knowledge_base_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒåˆ é™¤æ­¤çŸ¥è¯†åº“"
            )
        
        # åˆ é™¤çŸ¥è¯†åº“
        success = knowledge_service.delete_knowledge_base(knowledge_base_id, current_user.user_id)
        
        if not success:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {knowledge_base_id}"
            )
        
        logger.info(f"åˆ é™¤çŸ¥è¯†åº“æˆåŠŸ - ID: {knowledge_base_id}")
        
        return SuccessResponse(
            success=True,
            message="çŸ¥è¯†åº“åˆ é™¤æˆåŠŸ",
            data={
                "knowledge_base_id": knowledge_base_id,
                "deleted": True
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤çŸ¥è¯†åº“å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ é™¤çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

@router.post(
    "/{knowledge_base_id}/upload",
    response_model=SuccessResponse,
    summary="ä¸Šä¼ æ–‡æ¡£",
    description="ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“å¹¶è¿›è¡Œå¤„ç†"
)
async def upload_document(
    knowledge_base_id: str,
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    chunk_size: int = Form(1000),
    chunk_overlap: int = Form(200),
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    ä¸Šä¼ æ–‡æ¡£ç«¯ç‚¹
    
    Args:
        knowledge_base_id: çŸ¥è¯†åº“ID
        background_tasks: åå°ä»»åŠ¡ç®¡ç†å™¨
        file: ä¸Šä¼ çš„æ–‡ä»¶
        chunk_size: åˆ†å—å¤§å°
        chunk_overlap: åˆ†å—é‡å å¤§å°
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”
    """
    
    try:
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        existing_kb = knowledge_service.get_knowledge_base(knowledge_base_id)
        
        if not existing_kb:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}"
            )
        
        # æ£€æŸ¥æƒé™
        if existing_kb.created_by != current_user.user_id and not existing_kb.is_public:
            logger.warning(f"æƒé™æ‹’ç» - ç”¨æˆ·: {current_user.user_id} å°è¯•ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“: {knowledge_base_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒä¸Šä¼ æ–‡æ¡£åˆ°æ­¤çŸ¥è¯†åº“"
            )
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        allowed_extensions = ['.pdf', '.docx', '.txt', '.md', '.json']
        file_ext = '.' + file.filename.split('.')[-1].lower() if '.' in file.filename else ''
        
        if file_ext not in allowed_extensions:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ï¼Œæ”¯æŒçš„æ ¼å¼: {', '.join(allowed_extensions)}"
            )
        
        # ä¿å­˜æ–‡ä»¶
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # è§£ææ–‡æ¡£
            file_type = file_ext.lstrip('.')
            parsed_result = document_parser.parse_document(temp_file_path, file_type)
            
            # åˆ†å‰²æ–‡æœ¬
            text_splitter.chunk_size = chunk_size
            text_splitter.chunk_overlap = chunk_overlap
            
            chunks = text_splitter.split_text(
                parsed_result['content'],
                metadata={
                    'source_file': file.filename,
                    'file_type': file_type,
                    'file_size': len(content),
                    **parsed_result.get('metadata', {})
                }
            )
            
            # æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
            document_count = 0
            for chunk in chunks:
                document_item = knowledge_service.add_document(
                    knowledge_base_id,
                    {
                        'content': chunk.content,
                        'metadata': chunk.metadata,
                        'source_file': file.filename,
                        'chunk_index': chunk.chunk_index,
                        'total_chunks': chunk.total_chunks
                    }
                )
                
                if document_item:
                    document_count += 1
            
            logger.info(f"ä¸Šä¼ æ–‡æ¡£æˆåŠŸ - çŸ¥è¯†åº“: {knowledge_base_id}, æ–‡ä»¶: {file.filename}, æ–‡æ¡£å—: {document_count}")
            
            # åå°ä»»åŠ¡ï¼šå‘é‡åŒ–å¤„ç†
            background_tasks.add_task(
                self._vectorize_documents,
                knowledge_base_id,
                chunks
            )
            
            return SuccessResponse(
                success=True,
                message=f"æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼Œå¤„ç†äº† {document_count} ä¸ªæ–‡æ¡£å—",
                data={
                    "knowledge_base_id": knowledge_base_id,
                    "file_name": file.filename,
                    "file_size": len(content),
                    "chunks_processed": document_count,
                    "vectorization_queued": True
                }
            )
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file_path)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸Šä¼ æ–‡æ¡£å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ä¸Šä¼ æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )
    
    async def _vectorize_documents(self, knowledge_base_id: str, chunks: List):
        """åå°å‘é‡åŒ–æ–‡æ¡£"""
        try:
            # å‡†å¤‡æ–‡æ¡£æ•°æ®
            documents = []
            for chunk in chunks:
                documents.append({
                    'content': chunk.content,
                    'metadata': chunk.metadata
                })
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            result = rag_service.add_documents_to_knowledge_base(
                knowledge_base_id,
                documents
            )
            
            if result['success']:
                logger.info(f"å‘é‡åŒ–å®Œæˆ - çŸ¥è¯†åº“: {knowledge_base_id}, å¤„ç†æ–‡æ¡£: {result['processed_count']}")
            else:
                logger.error(f"å‘é‡åŒ–å¤±è´¥ - çŸ¥è¯†åº“: {knowledge_base_id}, é”™è¯¯: {result.get('error')}")
                
        except Exception as e:
            logger.error(f"åå°å‘é‡åŒ–å¼‚å¸¸: {str(e)}", exc_info=True)

@router.get(
    "/{knowledge_base_id}/documents",
    response_model=SuccessResponse,
    summary="è·å–çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨",
    description="è·å–æŒ‡å®šçŸ¥è¯†åº“çš„æ–‡æ¡£åˆ—è¡¨"
)
async def get_knowledge_base_documents(
    knowledge_base_id: str,
    page: int = 1,
    page_size: int = 20,
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    è·å–çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨ç«¯ç‚¹
    
    Args:
        knowledge_base_id: çŸ¥è¯†åº“ID
        page: é¡µç 
        page_size: æ¯é¡µå¤§å°
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”ï¼ŒåŒ…å«æ–‡æ¡£åˆ—è¡¨
    """
    
    try:
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        existing_kb = knowledge_service.get_knowledge_base(knowledge_base_id)
        
        if not existing_kb:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}"
            )
        
        # æ£€æŸ¥æƒé™
        if not existing_kb.is_public and existing_kb.created_by != current_user.user_id:
            logger.warning(f"æƒé™æ‹’ç» - ç”¨æˆ·: {current_user.user_id} å°è¯•è·å–çŸ¥è¯†åº“æ–‡æ¡£: {knowledge_base_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒè®¿é—®æ­¤çŸ¥è¯†åº“çš„æ–‡æ¡£"
            )
        
        # è®¡ç®—åç§»é‡
        offset = (page - 1) * page_size
        
        # è·å–æ–‡æ¡£åˆ—è¡¨
        documents = knowledge_service.get_documents(
            knowledge_base_id,
            limit=page_size,
            offset=offset
        )
        
        # è·å–æ–‡æ¡£æ€»æ•°
        total_docs = len(knowledge_service._documents.get(knowledge_base_id, []))
        
        # è®¡ç®—æ€»é¡µæ•°
        total_pages = (total_docs + page_size - 1) // page_size if total_docs > 0 else 1
        
        logger.info(f"è·å–çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨ - çŸ¥è¯†åº“: {knowledge_base_id}, æ•°é‡: {len(documents)}")
        
        return SuccessResponse(
            success=True,
            message="è·å–æ–‡æ¡£åˆ—è¡¨æˆåŠŸ",
            data={
                "items": [doc.dict() for doc in documents],
                "total": total_docs,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
                "has_next": page < total_pages,
                "has_prev": page > 1
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–æ–‡æ¡£åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

@router.post(
    "/{knowledge_base_id}/search",
    response_model=SuccessResponse,
    summary="æœç´¢çŸ¥è¯†åº“",
    description="åœ¨æŒ‡å®šçŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³å†…å®¹"
)
async def search_knowledge_base(
    knowledge_base_id: str,
    query: KnowledgeQuery,
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    æœç´¢çŸ¥è¯†åº“ç«¯ç‚¹
    
    Args:
        knowledge_base_id: çŸ¥è¯†åº“ID
        query: æœç´¢æŸ¥è¯¢
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”ï¼ŒåŒ…å«æœç´¢ç»“æœ
    """
    
    try:
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        existing_kb = knowledge_service.get_knowledge_base(knowledge_base_id)
        
        if not existing_kb:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}"
            )
        
        # æ£€æŸ¥æƒé™
        if not existing_kb.is_public and existing_kb.created_by != current_user.user_id:
            logger.warning(f"æƒé™æ‹’ç» - ç”¨æˆ·: {current_user.user_id} å°è¯•æœç´¢çŸ¥è¯†åº“: {knowledge_base_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒæœç´¢æ­¤çŸ¥è¯†åº“"
            )
        
        # æ‰§è¡Œæœç´¢
        search_results = rag_service.retrieve(
            query=query.query,
            knowledge_base_id=knowledge_base_id,
            top_k=query.top_k,
            score_threshold=query.score_threshold
        )
        
        logger.info(f"æœç´¢çŸ¥è¯†åº“ - çŸ¥è¯†åº“: {knowledge_base_id}, æŸ¥è¯¢: '{query.query[:50]}...', ç»“æœæ•°: {len(search_results)}")
        
        return SuccessResponse(
            success=True,
            message="æœç´¢æˆåŠŸ",
            data={
                "query": query.query,
                "knowledge_base_id": knowledge_base_id,
                "results": search_results,
                "total": len(search_results)
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æœç´¢çŸ¥è¯†åº“å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æœç´¢çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

@router.post(
    "/{knowledge_base_id}/query",
    response_model=SuccessResponse,
    summary="æŸ¥è¯¢çŸ¥è¯†åº“",
    description="ä½¿ç”¨RAGæŸ¥è¯¢çŸ¥è¯†åº“å¹¶è·å–å›ç­”"
)
async def query_knowledge_base(
    knowledge_base_id: str,
    query: KnowledgeQuery,
    current_user: UserContext = Depends(get_current_user)
) -> SuccessResponse:
    """
    æŸ¥è¯¢çŸ¥è¯†åº“ç«¯ç‚¹
    
    Args:
        knowledge_base_id: çŸ¥è¯†åº“ID
        query: æŸ¥è¯¢å†…å®¹
        current_user: å½“å‰ç”¨æˆ·ä¸Šä¸‹æ–‡
        
    Returns:
        SuccessResponse: æˆåŠŸå“åº”ï¼ŒåŒ…å«æŸ¥è¯¢ç»“æœ
    """
    
    try:
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        existing_kb = knowledge_service.get_knowledge_base(knowledge_base_id)
        
        if not existing_kb:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}"
            )
        
        # æ£€æŸ¥æƒé™
        if not existing_kb.is_public and existing_kb.created_by != current_user.user_id:
            logger.warning(f"æƒé™æ‹’ç» - ç”¨æˆ·: {current_user.user_id} å°è¯•æŸ¥è¯¢çŸ¥è¯†åº“: {knowledge_base_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒæŸ¥è¯¢æ­¤çŸ¥è¯†åº“"
            )
        
        # ä½¿ç”¨RAGæŸ¥è¯¢
        result = rag_service.answer_with_rag(
            query=query.query,
            knowledge_base_id=knowledge_base_id,
            top_k=query.top_k
        )
        
        logger.info(f"æŸ¥è¯¢çŸ¥è¯†åº“ - çŸ¥è¯†åº“: {knowledge_base_id}, æŸ¥è¯¢: '{query.query[:50]}...'")
        
        return SuccessResponse(
            success=result['success'],
            message="æŸ¥è¯¢å®Œæˆ",
            data=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢çŸ¥è¯†åº“å¼‚å¸¸: {str(e)}", exc_info=True)
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æŸ¥è¯¢çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

# å¯¼å‡ºè·¯ç”±å™¨
__all__ = ["router"]
```

#### 9. æ›´æ–°è·¯ç”±èšåˆå™¨
**æ–‡ä»¶ï¼š`app/api/router.py`**

```python
"""
APIè·¯ç”±èšåˆ
å°†æ‰€æœ‰APIè·¯ç”±é›†ä¸­æ³¨å†Œåˆ°FastAPIåº”ç”¨
"""

from fastapi import APIRouter

# å¯¼å…¥æ‰€æœ‰ç«¯ç‚¹è·¯ç”±
from app.api.v1.endpoints import (
    health,
    chat,
    employees,
    marketplace,
    knowledge  # æ–°å¢
)

# åˆ›å»ºAPI v1è·¯ç”±å™¨
api_v1_router = APIRouter(prefix="/api/v1")

# æ³¨å†Œå¥åº·æ£€æŸ¥è·¯ç”±
api_v1_router.include_router(health.router, prefix="/health", tags=["health"])

# æ³¨å†ŒèŠå¤©è·¯ç”±
api_v1_router.include_router(chat.router, prefix="/chat", tags=["chat"])

# æ³¨å†Œå‘˜å·¥ç®¡ç†è·¯ç”±
api_v1_router.include_router(employees.router, tags=["employees"])

# æ³¨å†Œå¸‚åœºå¹¿åœºè·¯ç”±
api_v1_router.include_router(marketplace.router, tags=["marketplace"])

# æ³¨å†ŒçŸ¥è¯†åº“ç®¡ç†è·¯ç”±ï¼ˆæ–°å¢ï¼‰
api_v1_router.include_router(knowledge.router, tags=["knowledge-bases"])

# åˆ›å»ºä¸»è·¯ç”±å™¨
router = APIRouter()

# åŒ…å«API v1è·¯ç”±å™¨
router.include_router(api_v1_router)

# å¯¼å‡ºè·¯ç”±å™¨
__all__ = ["router"]
```

### ğŸ¯ ç¬¬å…­é˜¶æ®µï¼šé›†æˆæµ‹è¯•ä¸å‰ç«¯å¯¹æ¥ï¼ˆ1-2å¤©ï¼‰

#### 10. åˆ›å»ºçŸ¥è¯†åº“æµ‹è¯•è„šæœ¬
**æ–‡ä»¶ï¼š`test_knowledge_system.py`**

```python
"""
çŸ¥è¯†åº“ç³»ç»Ÿæµ‹è¯•è„šæœ¬
"""

import requests
import json
import time

BASE_URL = "http://localhost:8000/api/v1"

def test_knowledge_base_crud():
    """æµ‹è¯•çŸ¥è¯†åº“CRUDæ“ä½œ"""
    print("=== æµ‹è¯•çŸ¥è¯†åº“CRUD ===")
    
    headers = {
        "X-Employee-ID": "test_emp",
        "X-User-ID": "test_user"
    }
    
    # 1. åˆ›å»ºçŸ¥è¯†åº“
    print("1. åˆ›å»ºçŸ¥è¯†åº“:")
    kb_data = {
        "name": "æµ‹è¯•çŸ¥è¯†åº“",
        "description": "ç”¨äºæµ‹è¯•çš„çŸ¥è¯†åº“",
        "tags": ["æµ‹è¯•", "æ–‡æ¡£"],
        "is_public": True,
        "category": "æµ‹è¯•åˆ†ç±»"
    }
    
    response = requests.post(f"{BASE_URL}/knowledge-bases", json=kb_data, headers=headers)
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        kb_info = result.get('data', {})
        kb_id = kb_info.get('id')
        print(f"   æˆåŠŸåˆ›å»ºçŸ¥è¯†åº“: {kb_info.get('name')} (ID: {kb_id})")
    else:
        print(f"   å¤±è´¥: {response.text}")
        return None
    
    # 2. è·å–çŸ¥è¯†åº“åˆ—è¡¨
    print("\n2. è·å–çŸ¥è¯†åº“åˆ—è¡¨:")
    response = requests.get(f"{BASE_URL}/knowledge-bases", headers=headers)
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        items = result.get('data', {}).get('items', [])
        print(f"   çŸ¥è¯†åº“æ•°é‡: {len(items)}")
    
    # 3. è·å–çŸ¥è¯†åº“è¯¦æƒ…
    print(f"\n3. è·å–çŸ¥è¯†åº“è¯¦æƒ… (ID: {kb_id}):")
    response = requests.get(f"{BASE_URL}/knowledge-bases/{kb_id}", headers=headers)
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        kb_detail = result.get('data', {})
        print(f"   çŸ¥è¯†åº“: {kb_detail.get('name')}")
        print(f"   æè¿°: {kb_detail.get('description')}")
        print(f"   æ–‡æ¡£æ•°é‡: {kb_detail.get('doc_count')}")
    
    # 4. æœç´¢çŸ¥è¯†åº“ï¼ˆæ— æ–‡æ¡£æ—¶ï¼‰
    print(f"\n4. æœç´¢çŸ¥è¯†åº“ (ID: {kb_id}):")
    search_data = {
        "query": "æµ‹è¯•æŸ¥è¯¢",
        "knowledge_base_id": kb_id,
        "top_k": 5,
        "score_threshold": 0.5
    }
    
    response = requests.post(f"{BASE_URL}/knowledge-bases/{kb_id}/search", json=search_data, headers=headers)
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        search_results = result.get('data', {}).get('results', [])
        print(f"   æœç´¢ç»“æœæ•°é‡: {len(search_results)}")
    
    # 5. æ›´æ–°çŸ¥è¯†åº“
    print(f"\n5. æ›´æ–°çŸ¥è¯†åº“ (ID: {kb_id}):")
    update_data = {
        "description": "æ›´æ–°åçš„æè¿°",
        "tags": ["æµ‹è¯•", "æ–‡æ¡£", "æ›´æ–°"]
    }
    
    response = requests.put(f"{BASE_URL}/knowledge-bases/{kb_id}", json=update_data, headers=headers)
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        print(f"   æ›´æ–°æˆåŠŸ: {result.get('message')}")
    
    return kb_id

def test_document_processing(kb_id: str):
    """æµ‹è¯•æ–‡æ¡£å¤„ç†"""
    print(f"\n=== æµ‹è¯•æ–‡æ¡£å¤„ç† (çŸ¥è¯†åº“: {kb_id}) ===")
    
    headers = {
        "X-Employee-ID": "test_emp",
        "X-User-ID": "test_user"
    }
    
    # åˆ›å»ºæµ‹è¯•æ–‡æœ¬æ–‡ä»¶
    import tempfile
    import os
    
    test_content = """è¿™æ˜¯ä¸€ä»½æµ‹è¯•æ–‡æ¡£ã€‚

åŒ…å«å¤šä¸ªæ®µè½å’Œå†…å®¹ã€‚

æµ‹è¯•æ–‡æ¡£çš„å†…å®¹å¯ä»¥ç”¨äºéªŒè¯æ–‡æ¡£å¤„ç†åŠŸèƒ½ã€‚

åŒ…æ‹¬æ–‡æœ¬è§£æã€åˆ†å‰²å’Œå‘é‡åŒ–ç­‰åŠŸèƒ½ã€‚

è¿™æ˜¯æœ€åä¸€æ®µå†…å®¹ã€‚"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        temp_file_path = f.name
    
    try:
        # ä¸Šä¼ æ–‡æ¡£
        print("1. ä¸Šä¼ æ–‡æ¡£:")
        with open(temp_file_path, 'rb') as file:
            files = {'file': ('test.txt', file, 'text/plain')}
            data = {'chunk_size': 500, 'chunk_overlap': 100}
            
            response = requests.post(
                f"{BASE_URL}/knowledge-bases/{kb_id}/upload",
                files=files,
                data=data,
                headers=headers
            )
        
        print(f"   çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print(f"   ä¸Šä¼ æˆåŠŸ: {result.get('message')}")
            
            # ç­‰å¾…å‘é‡åŒ–å¤„ç†
            print("   ç­‰å¾…å‘é‡åŒ–å¤„ç†...")
            time.sleep(3)
        else:
            print(f"   å¤±è´¥: {response.text}")
            return
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_file_path)
    
    # è·å–æ–‡æ¡£åˆ—è¡¨
    print("\n2. è·å–æ–‡æ¡£åˆ—è¡¨:")
    response = requests.get(f"{BASE_URL}/knowledge-bases/{kb_id}/documents", headers=headers)
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        items = result.get('data', {}).get('items', [])
        print(f"   æ–‡æ¡£æ•°é‡: {len(items)}")
        
        for i, doc in enumerate(items[:2]):
            print(f"   æ–‡æ¡£{i+1}: {doc.get('content', '')[:50]}...")
    
    # æœç´¢æ–‡æ¡£
    print("\n3. æœç´¢æ–‡æ¡£:")
    search_data = {
        "query": "æµ‹è¯•æ–‡æ¡£",
        "knowledge_base_id": kb_id,
        "top_k": 3,
        "score_threshold": 0.5
    }
    
    response = requests.post(f"{BASE_URL}/knowledge-bases/{kb_id}/search", json=search_data, headers=headers)
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        search_results = result.get('data', {}).get('results', [])
        print(f"   æœç´¢ç»“æœæ•°é‡: {len(search_results)}")
        
        for i, result in enumerate(search_results):
            print(f"   ç»“æœ{i+1}: åˆ†æ•°={result.get('score', 0):.3f}, å†…å®¹={result.get('content', '')[:50]}...")
    
    # RAGæŸ¥è¯¢
    print("\n4. RAGæŸ¥è¯¢:")
    query_data = {
        "query": "æ–‡æ¡£åŒ…å«ä»€ä¹ˆå†…å®¹ï¼Ÿ",
        "knowledge_base_id": kb_id,
        "top_k": 3,
        "score_threshold": 0.5
    }
    
    response = requests.post(f"{BASE_URL}/knowledge-bases/{kb_id}/query", json=query_data, headers=headers)
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        data = result.get('data', {})
        print(f"   æŸ¥è¯¢æˆåŠŸ: {data.get('success')}")
        print(f"   æ£€ç´¢ç»“æœæ•°: {data.get('result_count')}")

def test_system_integration():
    """æµ‹è¯•ç³»ç»Ÿé›†æˆ"""
    print("\n=== æµ‹è¯•ç³»ç»Ÿé›†æˆ ===")
    
    headers = {
        "X-Employee-ID": "test_emp",
        "X-User-ID": "test_user"
    }
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥
    print("1. ç³»ç»Ÿå¥åº·æ£€æŸ¥:")
    response = requests.get(f"{BASE_URL}/health")
    print(f"   çŠ¶æ€ç : {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        print(f"   ç³»ç»ŸçŠ¶æ€: {result.get('status')}")
    
    # æµ‹è¯•å„æ¨¡å—
    modules = [
        ("å¥åº·æ£€æŸ¥", "/health"),
        ("èŠå¤©", "/chat"),
        ("å‘˜å·¥", "/employees"),
        ("å¸‚åœº", "/marketplace/employees"),
        ("çŸ¥è¯†åº“", "/knowledge-bases")
    ]
    
    print("\n2. å„æ¨¡å—çŠ¶æ€:")
    for name, endpoint in modules:
        try:
            if endpoint == "/chat":
                # èŠå¤©éœ€è¦POSTè¯·æ±‚
                chat_data = {
                    "chat_request": {
                        "message": "æµ‹è¯•",
                        "employee_id": "mock_emp_001",
                        "conversation_id": None
                    }
                }
                response = requests.post(f"{BASE_URL}{endpoint}", json=chat_data, headers=headers)
            else:
                response = requests.get(f"{BASE_URL}{endpoint}", headers=headers)
            
            status = "âœ…" if response.status_code == 200 else "âŒ"
            print(f"   {status} {name}: {response.status_code}")
            
        except Exception as e:
            print(f"   âŒ {name}: è¿æ¥å¤±è´¥ - {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹çŸ¥è¯†åº“ç³»ç»Ÿæµ‹è¯•...")
    print("=" * 60)
    
    # æµ‹è¯•ç³»ç»Ÿé›†æˆ
    test_system_integration()
    
    # æµ‹è¯•çŸ¥è¯†åº“CRUD
    kb_id = test_knowledge_base_crud()
    
    if kb_id:
        # æµ‹è¯•æ–‡æ¡£å¤„ç†
        test_document_processing(kb_id)
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()
```

## ğŸ“Š å¼€å‘è®¡åˆ’æ—¶é—´è¡¨

| é˜¶æ®µ | å¤©æ•° | ä¸»è¦å†…å®¹ | äº¤ä»˜ç‰© |
|------|------|----------|--------|
| **ç¬¬ä¸€é˜¶æ®µ** | 1-2å¤© | æ•°æ®æ¨¡å‹ä¸åŸºç¡€æœåŠ¡ | 1. å®Œå–„çš„Pydanticæ¨¡å‹<br>2. çŸ¥è¯†åº“åŸºç¡€æœåŠ¡ |
| **ç¬¬äºŒé˜¶æ®µ** | 2-3å¤© | æ–‡æ¡£å¤„ç†æœåŠ¡ | 1. æ–‡æ¡£è§£æå™¨<br>2. æ–‡æœ¬åˆ†å‰²å™¨ |
| **ç¬¬ä¸‰é˜¶æ®µ** | 2-3å¤© | å‘é‡åŒ–ä¸å­˜å‚¨ | 1. åµŒå…¥æœåŠ¡<br>2. å‘é‡å­˜å‚¨(ChromaDB) |
| **ç¬¬å››é˜¶æ®µ** | 2-3å¤© | RAGæœåŠ¡ä¸API | 1. RAGæ£€ç´¢æœåŠ¡<br>2. APIç«¯ç‚¹ |
| **ç¬¬äº”é˜¶æ®µ** | 1-2å¤© | å‰ç«¯å¯¹æ¥ä¸æµ‹è¯• | 1. å‰ç«¯APIæœåŠ¡å±‚<br>2. å®Œæ•´æµ‹è¯•å¥—ä»¶ |

**æ€»è®¡ï¼š8-13ä¸ªå·¥ä½œæ—¥**

## ğŸš€ ç«‹å³è¡ŒåŠ¨æ­¥éª¤

### 1. **å®‰è£…ä¾èµ–**
```bash
cd backend-python-ai
pip install -r requirements.txt

# å®‰è£…çŸ¥è¯†åº“ç›¸å…³ä¾èµ–
pip install pypdf2 python-docx sentence-transformers chromadb
```

### 2. **åˆ›å»ºç›®å½•ç»“æ„**
```bash
mkdir -p app/services/processing
mkdir -p data/uploads
mkdir -p data/vector_db
```

### 3. **æŒ‰é¡ºåºå®ç°**
æŒ‰ç…§æˆ‘æä¾›çš„ä»£ç é¡ºåºï¼Œä¾æ¬¡åˆ›å»ºæ–‡ä»¶ï¼š

1. æ›´æ–° `app/models/schemas.py`
2. åˆ›å»º `app/services/knowledge_service.py`
3. åˆ›å»º `app/services/processing/document_parser.py`
4. åˆ›å»º `app/services/processing/text_splitter.py`
5. åˆ›å»º `app/services/processing/embedding_service.py`
6. åˆ›å»º `app/services/processing/vector_store.py`
7. åˆ›å»º `app/services/ai/rag_service.py`
8. åˆ›å»º `app/api/v1/endpoints/knowledge.py`
9. æ›´æ–° `app/api/router.py`
10. åˆ›å»º `test_knowledge_system.py`

### 4. **æµ‹è¯•ä¸éªŒè¯**
```bash
# å¯åŠ¨åç«¯æœåŠ¡
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œæµ‹è¯•
python test_knowledge_system.py
```

## ğŸ”§ æ³¨æ„äº‹é¡¹

### 1. **ä¾èµ–ç®¡ç†**
- **PyPDF2**ï¼šç”¨äºPDFè§£æ
- **python-docx**ï¼šç”¨äºWordæ–‡æ¡£è§£æ
- **sentence-transformers**ï¼šç”¨äºæ–‡æœ¬å‘é‡åŒ–
- **chromadb**ï¼šå‘é‡æ•°æ®åº“

### 2. **èµ„æºç®¡ç†**
- å¤§æ–‡ä»¶å¤„ç†éœ€è¦å¼‚æ­¥ä»»åŠ¡
- å‘é‡åŒ–è¿‡ç¨‹å¯èƒ½æ¶ˆè€—è¾ƒå¤šå†…å­˜
- å»ºè®®æ·»åŠ è¿›åº¦è¿½è¸ªå’Œé”™è¯¯å¤„ç†

### 3. **æ€§èƒ½ä¼˜åŒ–**
- æ‰¹é‡å¤„ç†æ–‡æ¡£
- ç¼“å­˜åµŒå…¥ç»“æœ
- å¼‚æ­¥æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†

### 4. **é”™è¯¯å¤„ç†**
- æ–‡ä»¶æ ¼å¼éªŒè¯
- ç½‘ç»œå¼‚å¸¸å¤„ç†
- å†…å­˜æº¢å‡ºä¿æŠ¤

## ğŸ“ˆ é¢„æœŸæˆæœ

å®ŒæˆçŸ¥è¯†åº“æœåŠ¡å¼€å‘åï¼Œä½ å°†æ‹¥æœ‰ï¼š

âœ… **å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹**ï¼šä¸Šä¼  â†’ è§£æ â†’ åˆ†å‰² â†’ å‘é‡åŒ– â†’ å­˜å‚¨  
âœ… **æ™ºèƒ½æ£€ç´¢èƒ½åŠ›**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è¯­ä¹‰æœç´¢  
âœ… **RAGé—®ç­”ç³»ç»Ÿ**ï¼šåŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”  
âœ… **RESTful API**ï¼šå®Œæ•´çš„CRUDæ“ä½œå’Œæœç´¢æ¥å£  
âœ… **å‰ç«¯å¯¹æ¥æ”¯æŒ**ï¼šä¸ºå‰ç«¯æä¾›å®Œæ•´çš„çŸ¥è¯†åº“ç®¡ç†ç•Œé¢